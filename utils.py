import os
import re
import json
import time
import math
import asyncio
import warnings
from typing import List, Dict, Tuple, Optional, AsyncIterator, Iterator, AsyncGenerator, Set, Any
from pathlib import Path
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
from rank_bm25 import BM25Okapi
import jieba
import docx
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM

# LlamaIndexæ ¸å¿ƒç»„ä»¶
from llama_index.core import (
    VectorStoreIndex,
    Document,
    Settings,
    StorageContext,
    ServiceContext
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.schema import NodeWithScore, QueryBundle, BaseNode
from llama_index.core.llms import (
    LLM, ChatMessage, CompletionResponse, 
    CompletionResponseGen, ChatResponse, ChatResponseGen
)

import os
import numpy as np
import faiss
from tqdm import tqdm
from typing import List, Optional
import jieba
from rank_bm25 import BM25Okapi
import hashlib
from openai import OpenAI
from vllm import LLM, SamplingParams

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

@dataclass
class EnhancedConfig:
    """å¢å¼ºé…ç½®ç±»"""
    
    # æ¨¡å‹è·¯å¾„
    EMBEDDING_MODEL = "/root/lanyun-fs/models/Qwen3-Embedding-0.6B"
    RERANKER_MODEL = "/root/lanyun-fs/models/Qwen3-Reranker-0.6B"
    # GENERATION_MODEL = "/root/lanyun-fs/models/Qwen3-0.6B"
    GENERATION_MODEL = "/root/lanyun-tmp/models/Qwen3-4B"
    
    # æŒ‡ä»¤
    EMBEDDING_INSTRUCTION = ""
    RERANKER_INSTRUCTION = ""
    
    # åˆ†å‰²å‚æ•° - æ”¹ä¸ºæ®µè½åˆ†å‰²
    PARAGRAPH_SEPARATOR = "\n\n"
    MAX_PARAGRAPH_LENGTH = 1024
    MIN_PARAGRAPH_LENGTH = 200
    
    # åˆ†å—å‚æ•°
    CHUNK_SIZE = 1024
    CHUNK_OVERLAP = 128
    MIN_CHUNK_SIZE = 100
    
    # åˆ†å‰²æ¨¡å¼é€‰æ‹©ï¼š'paragraph' æˆ– 'sentence'
    CHUNK_MODE = 'sentence'
    
    # æ£€ç´¢å‚æ•°
    SIMILARITY_TOP_K = 50
    BM25_TOP_K = 50
    RERANK_TOP_N = 25
    FINAL_TOP_K = 5

    include_knowledge_graph = True
    
    # ç»„ä»¶å¼€å…³
    QUERY_REWRITE_ENABLED = True
    QUERY_DECOMPOSE_ENABLED = False
    HYPO_ANSWER_ENABLED = False
    DEDUPLICATE_ENABLED = True
    RERANKER_ENABLED = True
    BM25_ENABLED = True
    
    # å¤šæŸ¥è¯¢å‚æ•°
    QUERY_REWRITE_NUM = 3
    
    # RRFå‚æ•°
    RRF_K = 60
    RRF_ENABLED = True
    # ä¸Šä¸‹æ–‡çª—å£
    CONTEXT_WINDOW = 1
    
    # ç”Ÿæˆå‚æ•°
    MAX_NEW_TOKENS = 5000
    TEMPERATURE = 0.3
    TOP_P = 1.0
    gpu_memory_utilization = 0.9
    max_model_len = 36000
    max_context_length = 30000
    # è®¾å¤‡é…ç½®
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # SimHashå»é‡å‚æ•°
    HASH_BITS = 64
    SIMILAR_THRESHOLD = 5
    MIN_TEXT_LENGTH = 100

class KnowledgeGraphProcessor:
    """çŸ¥è¯†å›¾è°±æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.documents = []
    
    def load_excel_knowledge_graph(self, excel_path: str) -> List[Document]:
        """åŠ è½½ExcelçŸ¥è¯†å›¾è°±æ•°æ®"""
        print(f"åŠ è½½çŸ¥è¯†å›¾è°±Excelæ–‡ä»¶: {excel_path}")
        
        try:
            # è¯»å–Excelæ–‡ä»¶
            data = pd.read_excel(excel_path, sheet_name='Sheet1', header=None)
            df = data.iloc[1:].copy()
            
            # è®¾ç½®åˆ—å
            df.columns = df.iloc[0].tolist()
            df = df.iloc[1:].copy()   
            # ç¡®ä¿éœ€è¦å¡«å……çš„åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
            fill_columns = ['ä¸€çº§', 'äºŒçº§', 'ä¸‰çº§']
            for col in fill_columns:
                df[col] = df[col].astype(str)  # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²
                df[col] = df[col].replace('nan', np.nan)  # å°†å­—ç¬¦ä¸²'nan'è½¬ä¸ºçœŸæ­£çš„NaN
                df[col] = df[col].replace('None', np.nan)  # å°†å­—ç¬¦ä¸²'None'è½¬ä¸ºçœŸæ­£çš„NaN
                df[col] = df[col].fillna(method='ffill')  # å‰å‘å¡«å……
            
            print("æ•°æ®å¡«å……å®Œæˆ!")

            knowledge_docs = []
            for index, row in df.iterrows():
                if pd.isna(row['ä¸€çº§']) or row['ä¸€çº§'] == 'ä¸€çº§':
                    continue

                doc_content = self._build_knowledge_document(row)
                if doc_content:
                    # åˆ›å»ºDocumentå¯¹è±¡
                    doc = Document(
                        text=doc_content,
                        metadata={
                            'file_name': 'æ‰¬å·å¸‚äººå·¥æ™ºèƒ½äº§ä¸šå›¾è°±.xlsx',
                            'folder_type': 'knowledge_graph',
                            'doc_id': f"kg_{index}",
                            'title': f"{row.get('ä¼ä¸š', '')} - {row.get('ä¸€çº§', '')}",
                            'industry_level_1': row.get('ä¸€çº§', ''),
                            'industry_level_2': row.get('äºŒçº§', ''),
                            'industry_level_3': row.get('ä¸‰çº§', ''),
                            'company': row.get('ä¼ä¸š', ''),
                            'business': row.get('æ¶‰åŠä¸šåŠ¡', ''),
                            'region': row.get('åœ°åŒº', ''),
                            'source_type': 'knowledge_graph'
                        }
                    )
                    knowledge_docs.append(doc)
            
            print(f"âœ“ çŸ¥è¯†å›¾è°±æ•°æ®åŠ è½½å®Œæˆ: {len(knowledge_docs)} ä¸ªä¼ä¸šèŠ‚ç‚¹")
            return knowledge_docs
            
        except Exception as e:
            print(f"âœ— åŠ è½½çŸ¥è¯†å›¾è°±Excelå¤±è´¥: {e}")
            return []
    
    def _build_knowledge_document(self, row) -> str:
        """æ„å»ºçŸ¥è¯†å›¾è°±æ–‡æ¡£å†…å®¹"""
        parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        if pd.notna(row.get('ä¼ä¸š')) and row['ä¼ä¸š']:
            parts.append(f"ä¼ä¸šåç§°ï¼š{row['ä¼ä¸š']}")
        
        if pd.notna(row.get('ä¸€çº§')) and row['ä¸€çº§']:
            parts.append(f"äº§ä¸šå±‚çº§ï¼š{row['ä¸€çº§']}")
        
        if pd.notna(row.get('äºŒçº§')) and row['äºŒçº§']:
            parts.append(f"ç»†åˆ†é¢†åŸŸï¼š{row['äºŒçº§']}")
        
        if pd.notna(row.get('ä¸‰çº§')) and row['ä¸‰çº§']:
            parts.append(f"å…·ä½“åˆ†ç±»ï¼š{row['ä¸‰çº§']}")
        
        if pd.notna(row.get('æ¶‰åŠä¸šåŠ¡')) and row['æ¶‰åŠä¸šåŠ¡']:
            parts.append(f"ä¸»è¥ä¸šåŠ¡ï¼š{row['æ¶‰åŠä¸šåŠ¡']}")
        
        if pd.notna(row.get('åœ°åŒº')) and row['åœ°åŒº']:
            parts.append(f"æ‰€åœ¨åœ°åŒºï¼š{row['åœ°åŒº']}")
        
        # æ„å»ºå®Œæ•´æè¿°
        if parts:
            return "\n".join(parts)
        return None
    
    def load_research_institutions(self, excel_path: str) -> List[Document]:
        """åŠ è½½ç ”ç©¶æœºæ„æ•°æ®ï¼ˆSheet2ï¼‰"""
        try:
            data = pd.read_excel(excel_path, sheet_name='Sheet2', header=None)
            df = data.copy()
            
            df.columns = ['region', 'institution']
            research_docs = []
            
            for index, row in df.iterrows():
                
                region = row['region']
                institution = row['institution']
                
                if institution:
                    doc_content = f"åœ°åŒºï¼š{region}\nç ”ç©¶æœºæ„ï¼š{institution}\nç±»å‹ï¼šäººå·¥æ™ºèƒ½ç ”ç©¶æœºæ„"
                    
                    doc = Document(
                        text=doc_content,
                        metadata={
                            'file_name': 'æ‰¬å·å¸‚äººå·¥æ™ºèƒ½äº§ä¸šå›¾è°±.xlsx',
                            'folder_type': 'knowledge_graph',
                            'doc_id': f"research_{index}",
                            'title': institution,
                            'region': region,
                            'institution': institution,
                            'source_type': 'research_institution'
                        }
                    )
                    research_docs.append(doc)
            print(f"âœ“ ç ”ç©¶æœºæ„æ•°æ®åŠ è½½å®Œæˆ: {len(research_docs)} ä¸ªæœºæ„")
            return research_docs
            
        except Exception as e:
            print(f"âœ— åŠ è½½ç ”ç©¶æœºæ„æ•°æ®å¤±è´¥: {e}")
            return []

class QueryRewriter:
    """æŸ¥è¯¢æ”¹å†™å™¨ - ç”ŸæˆåŒä¹‰é—®é¢˜æ‰©å±•æ£€ç´¢"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def rewrite_queries(self, original_query: str, num_queries: int = 2) -> List[str]:
        """ç”ŸæˆåŒä¹‰æŸ¥è¯¢"""
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆ{num_queries}ä¸ªä¸åŒä½†è¯­ä¹‰ç›¸ä¼¼çš„æŸ¥è¯¢é—®é¢˜ã€‚è¿™äº›æŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦è¡¨è¾¾ç›¸åŒçš„æ„æ€ï¼Œä»¥å¸®åŠ©æ£€ç´¢ç³»ç»Ÿæ‰¾åˆ°æ›´å…¨é¢çš„ç›¸å…³ä¿¡æ¯ã€‚

åŸå§‹é—®é¢˜ï¼š{original_query}

è¦æ±‚ï¼š
1. ä¿æŒæ ¸å¿ƒè¯­ä¹‰ä¸å˜
2. ä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼å’Œè§’åº¦
3. æ¶µç›–æ”¿ç­–æ–‡ä»¶å¯èƒ½ä½¿ç”¨çš„ä¸åŒæœ¯è¯­
4. æ¯ä¸ªæŸ¥è¯¢éƒ½åº”è¯¥æ˜¯å®Œæ•´çš„é—®é¢˜

è¯·ç›´æ¥è¾“å‡º{num_queries}ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢ä¸€è¡Œï¼š
"""
        messages = [ChatMessage(role="user", content=prompt)]
        try:
            response = self.llm.chat(messages)
            pattern = r'(.*?)</think>(.*)'
            match = re.search(pattern, response.content, re.DOTALL)
            if match:
                content = match.group(2).strip()
            else:
                content = response.content.strip()
        
            queries = [q.strip() for q in content.split('\n') if q.strip()]
            # ç¡®ä¿åŒ…å«åŸå§‹æŸ¥è¯¢
            if original_query not in queries:
                queries = [original_query] + queries
            return queries[:num_queries]
        except Exception as e:
            print(f"æŸ¥è¯¢æ”¹å†™å¤±è´¥: {e}")
            return [original_query]

class QueryDecomposer:
    """æŸ¥è¯¢åˆ†è§£å™¨ - å°†å¤æ‚æŸ¥è¯¢æ‹†åˆ†ä¸ºå­é—®é¢˜"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def decompose_query(self, complex_query: str) -> List[str]:
        """åˆ†è§£å¤æ‚æŸ¥è¯¢ä¸ºå­é—®é¢˜"""
        prompt = f"""
è¯·å°†ä»¥ä¸‹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸º2-4ä¸ªæ›´ç®€å•ã€æ›´å…·ä½“çš„å­é—®é¢˜ã€‚è¿™äº›å­é—®é¢˜åº”è¯¥æ¶µç›–åŸæŸ¥è¯¢çš„å„ä¸ªæ–¹é¢ï¼Œä¾¿äºåˆ†åˆ«æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚

å¤æ‚æŸ¥è¯¢ï¼š{complex_query}

è¦æ±‚ï¼š
1. æ¯ä¸ªå­é—®é¢˜åº”è¯¥ç‹¬ç«‹ä¸”å…·ä½“
2. å­é—®é¢˜ä¹‹é—´åº”è¯¥æœ‰é€»è¾‘å…³è”
3. æ¶µç›–åŸæŸ¥è¯¢çš„æ‰€æœ‰å…³é”®æ–¹é¢
4. ä¿æŒæ”¿ç­–æŸ¥è¯¢çš„ä¸“ä¸šæ€§

è¯·ç›´æ¥è¾“å‡ºå­é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜ä¸€è¡Œï¼š
"""
        messages = [ChatMessage(role="user", content=prompt)]
        try:
            response = self.llm.chat(messages)
            pattern = r'(.*?)</think>(.*)'
            match = re.search(pattern, response.content, re.DOTALL)
            if match:
                content = match.group(2).strip()
            else:
                content = response.content.strip()
                
            sub_queries = [q.strip() for q in content.split('\n') if q.strip()]
            return sub_queries if sub_queries else [complex_query]
        except Exception as e:
            print(f"æŸ¥è¯¢åˆ†è§£å¤±è´¥: {e}")
            return [complex_query]

class HypotheticalAnswerGenerator:
    """å‡è®¾ç­”æ¡ˆç”Ÿæˆå™¨ - ç”Ÿæˆç†æƒ³ç­”æ¡ˆç”¨äºæ£€ç´¢"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def generate_hypothetical_answer(self, query: str) -> str:
        """ä¸ºæŸ¥è¯¢ç”Ÿæˆå‡è®¾ç­”æ¡ˆ"""
        prompt = f"""
é’ˆå¯¹ä»¥ä¸‹æ”¿ç­–æŸ¥è¯¢é—®é¢˜ï¼Œè¯·ç”Ÿæˆä¸€ä¸ªç†æƒ³çš„ã€å…¨é¢çš„ç­”æ¡ˆã€‚è¿™ä¸ªç­”æ¡ˆå°†ç”¨äºæ£€ç´¢ç›¸å…³çš„æ”¿ç­–æ–‡æ¡£ã€‚

æŸ¥è¯¢ï¼š{query}

è¦æ±‚ï¼š
1. æƒ³è±¡ä¸€ä¸ªå®Œç¾çš„æ”¿ç­–ç­”æ¡ˆåº”è¯¥åŒ…å«å“ªäº›å†…å®¹
2. æ¶µç›–æŸ¥è¯¢çš„æ‰€æœ‰æ–¹é¢
3. ä½¿ç”¨æ”¿ç­–æ–‡æ¡£ä¸­å¯èƒ½å‡ºç°çš„ä¸“ä¸šæœ¯è¯­
4. ä¿æŒå®¢è§‚ã€å‡†ç¡®çš„æ”¿ç­–è¯­è¨€é£æ ¼

è¯·ç”Ÿæˆå‡è®¾ç­”æ¡ˆï¼š
"""
        messages = [ChatMessage(role="user", content=prompt)]
        
        try:
            response = self.llm.chat(messages)

            pattern = r'(.*?)</think>(.*)'
            match = re.search(pattern, response.content, re.DOTALL)
            if match:
                content = match.group(2).strip()
            else:
                content = response.content.strip()

            return content.strip()
        except Exception as e:
            print(f"å‡è®¾ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return query

class ParagraphSplitter:
    """æ®µè½åˆ†å‰²å™¨ - åŸºäºè¯­ä¹‰æ®µè½è€Œéå›ºå®šé•¿åº¦"""
    
    def __init__(self, max_length: int = 1000, min_length: int = 50):
        self.max_length = max_length
        self.min_length = min_length
    
    def split_document(self, document: Document) -> List[Document]:
        """å°†æ–‡æ¡£åˆ†å‰²ä¸ºæ®µè½"""
        # å¯¹äºçŸ¥è¯†å›¾è°±æ•°æ®ï¼Œä¸è¿›è¡Œåˆ†å‰²
        if document.metadata.get('source_type') in ['knowledge_graph', 'research_institution']:
            return [document]
            
        text = document.text
        paragraphs = []
        
        # æŒ‰æ®µè½åˆ†éš”ç¬¦åˆ†å‰²
        raw_paragraphs = text.split(EnhancedConfig.PARAGRAPH_SEPARATOR)
        
        for para in raw_paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # å¦‚æœæ®µè½è¿‡é•¿ï¼ŒæŒ‰å¥å­è¿›ä¸€æ­¥åˆ†å‰²
            if len(para) > self.max_length:
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?]', para)
                current_chunk = ""
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                        
                    if len(current_chunk) + len(sentence) <= self.max_length:
                        current_chunk += sentence + "ã€‚"
                    else:
                        if current_chunk and len(current_chunk) >= self.min_length:
                            paragraphs.append(current_chunk.strip())
                        current_chunk = sentence + "ã€‚"
                
                if current_chunk and len(current_chunk) >= self.min_length:
                    paragraphs.append(current_chunk.strip())
            else:
                if len(para) >= self.min_length:
                    paragraphs.append(para)
        
        # åˆ›å»ºæ–°çš„Documentå¯¹è±¡
        paragraph_docs = []
        for i, para_text in enumerate(paragraphs):
            new_doc = Document(
                text=para_text,
                metadata=document.metadata.copy()
            )
            new_doc.metadata['paragraph_id'] = i
            new_doc.metadata['chunk_type'] = 'paragraph'
            paragraph_docs.append(new_doc)
        
        return paragraph_docs

class SentenceChunkSplitter:
    """å¥å­åˆ†å—å™¨ - åŸºäºLlamaIndexçš„SentenceSplitter"""
    
    def __init__(self, chunk_size: int = 1024, chunk_overlap: int = 200, min_chunk_size: int = 100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.splitter = SentenceSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separator="\n"  # æŒ‰æ¢è¡Œç¬¦æ‹†åˆ†ï¼Œä¿ç•™æ®µè½ç»“æ„
        )
    
    def split_document(self, document: Document) -> List[Document]:
        """å°†æ–‡æ¡£åˆ†å‰²ä¸ºå¥å­å—"""
        # å¯¹äºçŸ¥è¯†å›¾è°±æ•°æ®ï¼Œä¸è¿›è¡Œåˆ†å‰²
        if document.metadata.get('source_type') in ['knowledge_graph', 'research_institution']:
            return [document]
            
        # ä½¿ç”¨LlamaIndexçš„SentenceSplitterï¼Œè¿™é‡Œsplit_textè¿”å›çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        text_chunks = self.splitter.split_text(document.text)
        
        # è½¬æ¢ä¸ºDocumentå¯¹è±¡
        chunk_docs = []
        for i, chunk_text in enumerate(text_chunks):
            if len(chunk_text) < self.min_chunk_size:
                continue
                
            new_doc = Document(
                text=chunk_text,
                metadata=document.metadata.copy()
            )
            new_doc.metadata['chunk_id'] = i
            new_doc.metadata['chunk_type'] = 'sentence'
            chunk_docs.append(new_doc)
        
        return chunk_docs

class DocumentSplitterFactory:
    """æ–‡æ¡£åˆ†å‰²å™¨å·¥å‚ - æ ¹æ®é…ç½®é€‰æ‹©åˆ†å‰²æ–¹æ³•"""
    
    @staticmethod
    def create_splitter() -> object:
        """åˆ›å»ºåˆ†å‰²å™¨å®ä¾‹"""
        if EnhancedConfig.CHUNK_MODE == 'paragraph':
            return ParagraphSplitter(
                max_length=EnhancedConfig.MAX_PARAGRAPH_LENGTH,
                min_length=EnhancedConfig.MIN_PARAGRAPH_LENGTH
            )
        elif EnhancedConfig.CHUNK_MODE == 'sentence':
            return SentenceChunkSplitter(
                chunk_size=EnhancedConfig.CHUNK_SIZE,
                chunk_overlap=EnhancedConfig.CHUNK_OVERLAP,
                min_chunk_size=EnhancedConfig.MIN_CHUNK_SIZE
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„CHUNK_MODE: {EnhancedConfig.CHUNK_MODE}")

class ReciprocalRankFusion:
    """å€’æ•°æ’åºèåˆç®—æ³•"""
    
    def __init__(self, k: int = 60):
        self.k = k
    
    def fuse(self, ranked_lists: List[List[NodeWithScore]]) -> List[NodeWithScore]:
        """èåˆå¤šä¸ªæ’åºåˆ—è¡¨"""
        if not ranked_lists:
            return []
        
        # åˆå§‹åŒ–åˆ†æ•°å­—å…¸
        scores = {}
        
        # å¯¹æ¯ä¸ªæ’åºåˆ—è¡¨è®¡ç®—RRFåˆ†æ•°
        for rank_list in ranked_lists:
            for rank, node in enumerate(rank_list):
                node_id = node.node.node_id
                if node_id not in scores:
                    scores[node_id] = 0.0
                scores[node_id] += 1.0 / (self.k + rank + 1)
        
        # åˆ›å»ºèåˆåçš„èŠ‚ç‚¹åˆ—è¡¨
        fused_nodes = []
        for node_id, score in scores.items():
            # æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹ï¼ˆå–ç¬¬ä¸€ä¸ªå‡ºç°çš„ä½ç½®ï¼‰
            for rank_list in ranked_lists:
                for node in rank_list:
                    if node.node.node_id == node_id:
                        fused_nodes.append(NodeWithScore(
                            node=node.node,
                            score=score
                        ))
                        break
                else:
                    continue
                break

        # æŒ‰åˆ†æ•°é™åºæ’åº
        fused_nodes.sort(key=lambda x: x.score, reverse=True)
        return fused_nodes

class SimHashDeduplicator:
    """SimHashå»é‡å™¨"""

    def __init__(self, config: EnhancedConfig):
        self.config = config
        self.hash_cache: Dict[str, int] = {}
        self.hash_set: Set[int] = set()
        
    def _tokenize(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        text = re.sub(r'[^\w\u4e00-\u9fa5]', ' ', text)
        words = jieba.cut(text)
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ'}
        tokens = [word for word in words if len(word) > 1 and word not in stop_words]
        return tokens
    
    def _get_word_hash(self, word: str) -> int:
        """è·å–è¯è¯­å“ˆå¸Œå€¼"""
        md5 = hashlib.md5(word.encode('utf-8'))
        return int(md5.hexdigest()[:16], 16)
    
    def _simhash(self, text: str) -> int:
        """è®¡ç®—SimHashå€¼"""
        if text in self.hash_cache:
            return self.hash_cache[text]
            
        tokens = self._tokenize(text)
        if not tokens:
            return 0
            
        vector = [0] * self.config.HASH_BITS
        
        for token in tokens:
            token_hash = self._get_word_hash(token)
            for i in range(self.config.HASH_BITS):
                bit_mask = 1 << i
                if token_hash & bit_mask:
                    vector[i] += 1
                else:
                    vector[i] -= 1
        
        fingerprint = 0
        for i in range(self.config.HASH_BITS):
            if vector[i] > 0:
                fingerprint |= 1 << i
                
        self.hash_cache[text] = fingerprint
        return fingerprint

    def _hamming_distance(self, hash1: int, hash2: int) -> int:
        """è®¡ç®—æ±‰æ˜è·ç¦»"""
        xor_result = hash1 ^ hash2
        distance = 0
        while xor_result:
            distance += 1
            xor_result &= xor_result - 1
        return distance
    
    def deduplicate_nodes(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:
        """èŠ‚ç‚¹å»é‡"""
        unique_nodes = []
        seen_hashes = set()
        
        for node in nodes:
            node_text = node.node.text
            if len(node_text) < self.config.MIN_TEXT_LENGTH:
                unique_nodes.append(node)
                continue
                
            node_hash = self._simhash(node_text)
            is_duplicate = False
            
            for existing_hash in seen_hashes:
                if self._hamming_distance(node_hash, existing_hash) <= self.config.SIMILAR_THRESHOLD:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_nodes.append(node)
                seen_hashes.add(node_hash)
        
        return unique_nodes

class KeyTermExtractor:
    """å…³é”®ä¿¡æ¯æå–å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    @staticmethod
    def extract_key_terms(query: str) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬"""
        key_terms = {
            'years': [],
            'numbers': [],
            'policy_names': [],
            'locations': [],
            'key_entities': [],
            'time_range': None,
            'key_phrases': []  # æ–°å¢ï¼šå…³é”®çŸ­è¯­
        }

        # æå–å¹´ä»½ï¼ˆ4ä½æ•°å­—ï¼‰
        years = re.findall(r'(?<!\d)(19\d{2}|20\d{2})(?!\d)', query)
        key_terms['years'] = list(set(years))

        # æå–æ—¶é—´èŒƒå›´ï¼ˆå¢å¼ºæ¨¡å¼ï¼‰
        time_range_patterns = [
            r'(\b(19|20)\d{2})\s*[-è‡³]\s*(\b(19|20)\d{2})',  # 2023-2025 æˆ– 2023è‡³2025
            r'(\b(19|20)\d{2})å¹´\s*(?:[-è‡³]\s*)?(\b(19|20)\d{2})å¹´',  # 2023å¹´-2025å¹´
        ]
        
        for pattern in time_range_patterns:
            time_range_match = re.search(pattern, query)
            if time_range_match:
                start_year = int(time_range_match.group(1))
                end_year = int(time_range_match.group(3))
                key_terms['time_range'] = {'start': start_year, 'end': end_year}
                break
        
        # æå–æ•°å­—ï¼ˆåŒ…æ‹¬å°æ•°å’Œç™¾åˆ†æ¯”ï¼‰
        numbers = re.findall(r'\b\d+(?:\.\d+)?%?\b', query)
        key_terms['numbers'] = list(set(numbers))
        
        # æå–æ”¿ç­–åç§°ï¼ˆä¹¦åå·å†…å†…å®¹ï¼‰
        policy_names = re.findall(r'ã€Š([^ã€Šã€‹]+)ã€‹', query)
        key_terms['policy_names'] = policy_names
        
        # æå–åœ°ç‚¹ï¼ˆæ‰¬å·ç›¸å…³ - å¢å¼ºï¼‰
        locations = re.findall(r'(æ±Ÿéƒ½åŒº|é«˜é‚®å¸‚|ä»ªå¾å¸‚|å¹¿é™µåŒº|é‚—æ±ŸåŒº|å®åº”å¿|æ™¯åŒº|ç»æµå¼€å‘åŒº|ç”Ÿæ€ç§‘æŠ€æ–°åŸ|ç»æµæŠ€æœ¯å¼€å‘åŒº|å¼€å‘åŒº|æ‰¬å·)', query)
        key_terms['locations'] = list(set(locations))
        
        # æå–å…³é”®å®ä½“
        entities = re.findall(r'(ä»»åŠ¡æ¸…å•|ä¸»è¦ç›®æ ‡|å®æ–½æ–¹æ¡ˆ|æ”¿ç­–|é€šçŸ¥|åŠæ³•|æ¡ä¾‹|è§„å®š|ç»†åˆ™|æŒ‡å—|è§„åˆ’|è®¡åˆ’|æ–¹æ¡ˆ|å·¥ä½œè¦ç‚¹|é‡ç‚¹å·¥ä½œ)', query)
        key_terms['key_entities'] = list(set(entities))
        
        # æå–å…³é”®çŸ­è¯­
        phrases = re.findall(r'(?:^|\s)([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6}?(?:ä»»åŠ¡|ç›®æ ‡|è®¡åˆ’|æ–¹æ¡ˆ|æ”¿ç­–|æªæ–½|å·¥ä½œ))', query)
        key_terms['key_phrases'] = phrases
        
        return key_terms
    
    @staticmethod
    def calculate_semantic_similarity(file_name: str, query: str) -> float:
        """è®¡ç®—æ–‡æ¡£åç§°ä¸æŸ¥è¯¢çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„æ¨¡å‹
        file_words = set(re.findall(r'[\u4e00-\u9fff]+', file_name))
        query_words = set(re.findall(r'[\u4e00-\u9fff]+', query))
        
        if not file_words or not query_words:
            return 0.0
            
        intersection = file_words & query_words
        union = file_words | query_words
        
        return len(intersection) / len(union) if union else 0.0
    
    @staticmethod
    def calculate_key_term_boost(file_name: str, key_terms: Dict[str, Any], query: str) -> Tuple[float, Dict[str, float]]:
        """è®¡ç®—æ–‡æ¡£åç§°çš„å…³é”®ä¿¡æ¯åŒ¹é…åº¦æå‡ - è¿”å›è¯¦ç»†çš„åˆ†é¡¹å¾—åˆ†"""
        boost = 0.0
        penalty = 0.0
        detailed_scores = {
            'semantic_similarity': 0.0,
            'year_match': 0.0,
            'policy_match': 0.0,
            'location_match': 0.0,
            'phrase_match': 0.0,
            'time_range_match': 0.0,
            'location_penalty': 0.0,
            'time_penalty': 0.0,
            'year_mismatch_penalty': 0.0
        }
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åŸºç¡€åˆ† (æƒé‡æœ€é«˜)
        semantic_similarity = KeyTermExtractor.calculate_semantic_similarity(file_name, query)
        semantic_boost = semantic_similarity * 0.4  # æé«˜è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
        boost += semantic_boost
        detailed_scores['semantic_similarity'] = semantic_boost

        # æå–æ–‡æ¡£åç§°ä¸­çš„å¹´ä»½
        file_years = re.findall(r'(?<!\d)(19\d{2}|20\d{2})(?!\d)', file_name)
        file_years_int = [int(year) for year in file_years] if file_years else []

        # 2. å¹´ä»½åŒ¹é… (åˆ†çº§æƒé‡)
        year_boost = 0.0
        year_mismatch_penalty = 0.0

        if key_terms['years']:  # æŸ¥è¯¢ä¸­å­˜åœ¨å¹´ä»½
            matched_years = []
            for year in key_terms['years']:
                if year in file_name:
                    matched_years.append(year)
                    if file_name.startswith(year) or f"{year}å¹´" in file_name:
                        year_boost += 1.0  # é‡è¦ä½ç½®å¹´ä»½
                    else:
                        year_boost += 1.0  # æ™®é€šä½ç½®å¹´ä»½

            # å¹´ä»½ä¸åŒ¹é…æƒ©ç½šï¼šæŸ¥è¯¢ä¸­æœ‰å¹´ä»½ä½†æ–‡æ¡£åç§°ä¸­çš„å¹´ä»½éƒ½ä¸åŒ¹é…
            if not matched_years and file_years:  # æ–‡æ¡£æœ‰å¹´ä»½ä½†ä¸æŸ¥è¯¢ä¸åŒ¹é…
                year_mismatch_penalty += 3.0
                print(f"  âœ— å¹´ä»½ä¸åŒ¹é…: æŸ¥è¯¢æŒ‡å®šå¹´ä»½{key_terms['years']}ï¼Œä½†æ–‡æ¡£åç§°å¹´ä»½{file_years}ä¸åŒ¹é…, -2")

            # elif not matched_years and key_terms['years']:  # æŸ¥è¯¢æœ‰å¹´ä»½ä½†æ–‡æ¡£æ— å¹´ä»½
            #     year_mismatch_penalty += 0.1
            #     print(f"  âœ— å¹´ä»½ç¼ºå¤±: æŸ¥è¯¢æŒ‡å®šå¹´ä»½{key_terms['years']}ï¼Œä½†æ–‡æ¡£åç§°æ— å¹´ä»½ä¿¡æ¯, -0.4")

        if year_boost > 0:
            year_boost = min(year_boost, 2.0)  # é™åˆ¶æœ€å¤§å¥–åŠ±
            boost += year_boost
            detailed_scores['year_match'] = year_boost

        # 3. æ”¿ç­–åç§°åŒ¹é…
        policy_boost = 0.0
        for policy_name in key_terms['policy_names']:
            if policy_name in file_name:
                policy_boost += 0.5

        if policy_boost > 0:
            policy_boost = min(policy_boost, 1.8)
            boost += policy_boost
            detailed_scores['policy_match'] = policy_boost

        # 4. åœ°ç‚¹åŒ¹é…
        location_boost = 0.0
        for location in key_terms['locations']:
            if location in file_name:
                location_boost += 0.3

        if location_boost > 0:
            location_boost = min(location_boost, 0.9)
            boost += location_boost
            detailed_scores['location_match'] = location_boost

        # 5. å…³é”®çŸ­è¯­åŒ¹é…
        phrase_boost = 0.0
        for phrase in key_terms['key_phrases']:
            if phrase in file_name:
                phrase_boost += 0.4

        if phrase_boost > 0:
            phrase_boost = min(phrase_boost, 1.2)
            boost += phrase_boost
            detailed_scores['phrase_match'] = phrase_boost

        # 6. æ—¶é—´èŒƒå›´åŒ¹é… (ä¼˜åŒ–é€»è¾‘)
        time_range_boost = 0.0
        time_penalty = 0.0

        if key_terms['time_range']:
            start_year = key_terms['time_range']['start']
            end_year = key_terms['time_range']['end']

            if file_years_int:
                max_file_year = max(file_years_int) if file_years_int else 0

                # ç²¾ç¡®åŒ¹é…å¥–åŠ±
                in_range_years = [year for year in file_years_int if start_year <= year <= end_year]
                if in_range_years:
                    time_range_boost += 0.8
                else:
                    # ç›¸é‚»å¹´ä»½å¥–åŠ± (Â±1-2å¹´)
                    adjacent_years = [
                        year for year in file_years_int 
                        if start_year - 1 <= year <= end_year + 1
                    ]
                    if adjacent_years:
                        time_range_boost += 0.1
                    else:
                        # æ—¶é—´å®Œå…¨ä¸ç›¸å…³æƒ©ç½š
                        if max_file_year < start_year - 5:  # è¿‡äºé™ˆæ—§
                            time_penalty += 3.0
                        elif max_file_year > end_year + 5:  # è¿‡äºè¶…å‰
                            time_penalty += 3.0

        boost += time_range_boost
        penalty += time_penalty
        detailed_scores['time_range_match'] = time_range_boost
        detailed_scores['time_penalty'] = time_penalty

        # 7. åœ°ç‚¹ä¸åŒ¹é…æƒ©ç½š
        location_penalty = 0.0
        if key_terms['locations']:
            location_matched = any(location in file_name for location in key_terms['locations'])
            if not location_matched:
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŒçœä»½å…¶ä»–åŸå¸‚
                jiangsu_cities = ['å—äº¬', 'å—äº¬å¸‚', 'è‹å·', 'è‹å·å¸‚', 'æ— é”¡', 'æ— é”¡å¸‚', 'å¸¸å·', 'å¸¸å·å¸‚', 
                                 'é•‡æ±Ÿ', 'é•‡æ±Ÿå¸‚', 'å—é€š', 'å—é€šå¸‚', 'æ³°å·', 'æ³°å·å¸‚', 'å¾å·', 'å¾å·å¸‚', 
                                 'å®¿è¿', 'å®¿è¿å¸‚', 'è¿äº‘æ¸¯', 'è¿äº‘æ¸¯å¸‚', 'æ·®å®‰', 'æ·®å®‰å¸‚', 'ç›åŸ', 'ç›åŸå¸‚']
                file_other_locations = [city for city in jiangsu_cities if city in file_name]

                if file_other_locations:
                    location_penalty += 3.0 # åŒçœä»½å…¶ä»–åŸå¸‚ï¼Œè½»åº¦æƒ©ç½š
                else:
                    location_penalty += 3.0  # å®Œå…¨ä¸åŒçš„åœ°ç‚¹ï¼Œä¸­åº¦æƒ©ç½š

        penalty += location_penalty
        detailed_scores['location_penalty'] = location_penalty

        # 8. å¹´ä»½ä¸åŒ¹é…æƒ©ç½š
        penalty += year_mismatch_penalty
        detailed_scores['year_mismatch_penalty'] = year_mismatch_penalty

        # è®¡ç®—æœ€ç»ˆè°ƒæ•´åˆ†æ•°
        final_adjustment = boost - penalty

        # é™åˆ¶è°ƒæ•´èŒƒå›´åœ¨åˆç†åŒºé—´ [-1.0, 2.0]
        final_adjustment = max(-3.0, min(3.0, final_adjustment))

        return final_adjustment, detailed_scores
    
    @staticmethod
    def apply_key_term_adjustment(nodes: List[NodeWithScore], query: str) -> List[NodeWithScore]:
        """åº”ç”¨å…³é”®ä¿¡æ¯è°ƒæ•´åˆ°èŠ‚ç‚¹åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not nodes:
            return nodes
            
        key_terms = KeyTermExtractor.extract_key_terms(query)
        
        # æ‰“å°æå–çš„å…³é”®ä¿¡æ¯
        print(f"\n=== å…³é”®ä¿¡æ¯æå– ===")
        print(f"æŸ¥è¯¢: {query}")
        for category, terms in key_terms.items():
            if terms or (category == 'time_range' and terms is not None):
                print(f"{category}: {terms}")
        
        print(f"\n=== å…³é”®ä¿¡æ¯è°ƒæ•´ ===")
        
        adjusted_nodes = []
        adjustment_details = []
        
        for i, node in enumerate(nodes):
            original_score = node.score
            file_name = node.node.metadata.get('file_name', '')
            adjustment, detailed_scores = KeyTermExtractor.calculate_key_term_boost(file_name, key_terms, query)
            
            # æ™ºèƒ½è¯„åˆ†èåˆç­–ç•¥
            new_score = KeyTermExtractor._intelligent_score_fusion(original_score, adjustment, detailed_scores, file_name)
            
            # åˆ›å»ºæ–°èŠ‚ç‚¹
            adjusted_node = NodeWithScore(
                node=node.node,
                score=new_score
            )
            adjusted_nodes.append(adjusted_node)
            adjustment_details.append((i, original_score, new_score, adjustment))
            
            if adjustment != 0:
                print(f"èŠ‚ç‚¹ {i+1}: åŸå§‹åˆ†æ•° {original_score:.3f} -> è°ƒæ•´å {new_score:.3f} (è°ƒæ•´: {adjustment:+.2f})")
        
        # é‡æ–°æ’åº
        adjusted_nodes.sort(key=lambda x: x.score, reverse=True)
        
        # æ‰“å°è°ƒæ•´ç»Ÿè®¡
        if nodes:
            original_top_score = nodes[0].score
            adjusted_top_score = adjusted_nodes[0].score
            score_change = adjusted_top_score - original_top_score

            print(f"\n=== è°ƒæ•´ç»Ÿè®¡ ===")
            print(f"æœ€é«˜åˆ†: {original_top_score:.3f} -> {adjusted_top_score:.3f} ({score_change:+.3f})")

            # æ˜¾ç¤ºåˆ†æ•°åˆ†å¸ƒ
            original_scores = [node.score for node in nodes[:5]]
            adjusted_scores = [node.score for node in adjusted_nodes[:5]]
            print(f"å‰5ååŸå§‹åˆ†æ•°: {[f'{s:.3f}' for s in original_scores]}")
            print(f"å‰5åè°ƒæ•´åˆ†æ•°: {[f'{s:.3f}' for s in adjusted_scores]}")

        return adjusted_nodes

    @staticmethod
    def _intelligent_score_fusion(original_score: float, total_boost: float, 
                                detailed_scores: Dict[str, float], file_name: str) -> float:
        """æ™ºèƒ½è¯„åˆ†èåˆç­–ç•¥"""
        
        # åŸºç¡€è°ƒæ•´ï¼šä½¿ç”¨sigmoidå‡½æ•°è¿›è¡Œå¹³æ»‘è°ƒæ•´
        def sigmoid_adjustment(x):
            return 4 / (1 + math.exp(-2 * x)) - 2  # è¾“å‡ºèŒƒå›´[-2, 2]
        
        # 1. å¯¹äºé«˜åŸå§‹åˆ†æ•°ï¼Œè°ƒæ•´è¦æ›´è°¨æ…
        if original_score > 1.5:
            adjustment_factor = 0.3  # é«˜åˆ†æ•°æ–‡æ¡£è°ƒæ•´å¹…åº¦è¾ƒå°
        elif original_score > 0.8:
            adjustment_factor = 0.6  # ä¸­ç­‰åˆ†æ•°æ–‡æ¡£è°ƒæ•´å¹…åº¦ä¸­ç­‰
        else:
            adjustment_factor = 0.8  # ä½åˆ†æ•°æ–‡æ¡£è°ƒæ•´å¹…åº¦è¾ƒå¤§
        
        # 2. ä½¿ç”¨sigmoidå‡½æ•°å¹³æ»‘è°ƒæ•´å€¼
        smoothed_boost = sigmoid_adjustment(total_boost) * adjustment_factor
        
        # 3. è€ƒè™‘å„é¡¹å¾—åˆ†çš„æƒé‡åˆ†å¸ƒ
        positive_components = sum([
            detailed_scores['semantic_similarity'],
            detailed_scores['year_match'],
            detailed_scores['policy_match'],
            detailed_scores['location_match'],
            detailed_scores['phrase_match'],
            detailed_scores['time_range_match']
        ])
        
        negative_components = sum([
            detailed_scores['location_penalty'],
            detailed_scores['time_penalty']
        ])
        
        # 4. è®¡ç®—ç½®ä¿¡åº¦æƒé‡
        confidence_weight = min(1.0, positive_components / 3.0)  # æ­£é¡¹å¾—åˆ†è¶Šé«˜ï¼Œç½®ä¿¡åº¦è¶Šé«˜
        
        # 5. æœ€ç»ˆè°ƒæ•´è®¡ç®—
        if total_boost > 0:
            # æ­£å‘è°ƒæ•´ï¼šåŸºäºç½®ä¿¡åº¦åŠ æƒ
            final_adjustment = smoothed_boost * confidence_weight
            new_score = original_score * (1 + final_adjustment)
        else:
            # è´Ÿå‘è°ƒæ•´ï¼šæ›´è°¨æ…ï¼Œè€ƒè™‘åŸå§‹åˆ†æ•°
            penalty_severity = 0.5 if original_score > 0.6 else 0.3
            final_adjustment = smoothed_boost * penalty_severity
            new_score = original_score * (1 + final_adjustment)
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†… [0, 1]
        new_score = max(0.0, min(1.0, new_score))
        
        return new_score

    @staticmethod
    def _print_adjustment_details(file_name: str, original_score: float, 
                                new_score: float, total_boost: float, 
                                detailed_scores: Dict[str, float]):
        """æ‰“å°è°ƒæ•´è¯¦æƒ…"""
        print(f"\nğŸ“„ æ–‡æ¡£: {file_name}")
        print(f"   åŸå§‹åˆ†æ•°: {original_score:.3f} â†’ æ–°åˆ†æ•°: {new_score:.3f}")
        print(f"   æ€»è°ƒæ•´å€¼: {total_boost:.2f}")
        
        # æ‰“å°æ­£é¡¹å¾—åˆ†
        positive_scores = {k: v for k, v in detailed_scores.items() if v > 0 and 'penalty' not in k}
        if positive_scores:
            print("   âœ… å¥–åŠ±é¡¹:")
            for key, value in positive_scores.items():
                if value > 0:
                    print(f"     - {key}: +{value:.2f}")
        
        # æ‰“å°è´Ÿé¡¹å¾—åˆ†
        negative_scores = {k: v for k, v in detailed_scores.items() if v > 0 and 'penalty' in k}
        if negative_scores:
            print("   âŒ æƒ©ç½šé¡¹:")
            for key, value in negative_scores.items():
                if value > 0:
                    print(f"     - {key}: -{value:.2f}")

    @staticmethod
    def _print_fusion_statistics(adjustment_stats: List[dict], filtered_nodes: List[NodeWithScore]):
        """æ‰“å°èåˆç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n=== æ™ºèƒ½è¯„åˆ†èåˆç»Ÿè®¡ ===")
        print(f"å¤„ç†èŠ‚ç‚¹æ€»æ•°: {len(adjustment_stats)}")
        
        if adjustment_stats:
            avg_original = sum(stat['original_score'] for stat in adjustment_stats) / len(adjustment_stats)
            avg_new = sum(stat['new_score'] for stat in adjustment_stats) / len(adjustment_stats)
            avg_adjustment = sum(stat['adjustment'] for stat in adjustment_stats) / len(adjustment_stats)
            
            print(f"å¹³å‡åŸå§‹åˆ†æ•°: {avg_original:.3f}")
            print(f"å¹³å‡æ–°åˆ†æ•°: {avg_new:.3f}")
            print(f"å¹³å‡è°ƒæ•´å¹…åº¦: {avg_adjustment:+.3f}")
            
            # è°ƒæ•´å¹…åº¦åˆ†å¸ƒ
            positive_adjustments = [stat for stat in adjustment_stats if stat['adjustment'] > 0]
            negative_adjustments = [stat for stat in adjustment_stats if stat['adjustment'] < 0]
            no_adjustments = [stat for stat in adjustment_stats if stat['adjustment'] == 0]
            
            print(f"åˆ†æ•°æå‡æ–‡æ¡£: {len(positive_adjustments)}ä¸ª")
            print(f"åˆ†æ•°é™ä½æ–‡æ¡£: {len(negative_adjustments)}ä¸ª")
            print(f"åˆ†æ•°ä¸å˜æ–‡æ¡£: {len(no_adjustments)}ä¸ª")
        
        if filtered_nodes:
            print(f"è°ƒæ•´åæœ€é«˜åˆ†: {filtered_nodes[0].score:.3f}")
            print(f"è°ƒæ•´åæœ€ä½åˆ†: {filtered_nodes[-1].score:.3f}")

class Qwen3Embedding:
    """Qwen3 Embeddingæ¨¡å‹å°è£…"""

    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-8B", instruction: str = ""):
        self.model_name = model_name
        self.instruction = instruction
        self.model = None
        self.tokenizer = None
        self._initialize_model()

    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print(f"åŠ è½½Qwen3 Embeddingæ¨¡å‹: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=EnhancedConfig.DEVICE
        )
        self.model.eval()
        print("Qwen3 Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")

    def get_query_embedding(self, query: str) -> List[float]:
        """è·å–æŸ¥è¯¢åµŒå…¥"""
        if self.instruction:
            query = f"{self.instruction}\n{query}"

        with torch.no_grad():
            inputs = self.tokenizer(
                query,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=8192
            ).to(EnhancedConfig.DEVICE)
    
            outputs = self.model(**inputs)
            # ä½¿ç”¨å¹³å‡æ± åŒ–è·å–åµŒå…¥
            embeddings = self._mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])
            return embeddings[0].cpu().numpy().tolist()

    def get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """è·å–æ–‡æœ¬åµŒå…¥åˆ—è¡¨"""
        if self.instruction:
            texts = [f"{self.instruction}\n{text}" for text in texts]

        with torch.no_grad():
            inputs = self.tokenizer(
                texts,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=8192
            ).to(EnhancedConfig.DEVICE)

            outputs = self.model(**inputs)
            # ä½¿ç”¨å¹³å‡æ± åŒ–è·å–åµŒå…¥
            embeddings = self._mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])
            return embeddings.cpu().numpy().tolist()

    def _mean_pooling(self, model_output, attention_mask):
        """å¹³å‡æ± åŒ–è·å–å¥å­åµŒå…¥"""
        token_embeddings = model_output
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9)

class Qwen3Reranker:
    """Qwen3 Rerankeræ¨¡å‹å°è£…"""

    def __init__(self, model_name: str = "Qwen/Qwen3-Reranker-8B", instruction: str = ""):
        self.model_name = model_name
        self.instruction = instruction
        self.model = None
        self.tokenizer = None
        self._initialize_model()

    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""

        print(f"åŠ è½½Qwen3 Rerankeræ¨¡å‹: {self.model_name}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True)
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=EnhancedConfig.DEVICE)
        
        self.model.eval()
        print("Qwen3 Rerankeræ¨¡å‹åŠ è½½æˆåŠŸ")

    def compute_score(self, pairs: List[Tuple[str, str]]) -> List[float]:
        """è®¡ç®—æŸ¥è¯¢-æ–‡æ¡£å¯¹çš„ç›¸å…³æ€§åˆ†æ•°"""
        scores = []
        for query, document in pairs:

            q_d_data = f"æŸ¥è¯¢: {query}\næ–‡æ¡£: {document}"
            messages = []
            messages.append({"role": "system", "content":self.instruction})
            messages.append({"role": "assistant","content":q_d_data})
            
            messages_inputs = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors="pt")
            
            inputs = self.tokenizer(messages_inputs, return_tensors="pt").to(EnhancedConfig.DEVICE)
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits[:, -1, :]  # å–æœ€åä¸€ä¸ªtokençš„logits

                yes_token_id = self.tokenizer.encode("yes", add_special_tokens=False)[0]
                no_token_id = self.tokenizer.encode("no", add_special_tokens=False)[0]

                yes_logit = logits[0, yes_token_id].item()
                no_logit = logits[0, no_token_id].item()
                exp_yes = np.exp(yes_logit)
                exp_no = np.exp(no_logit)
                score = exp_yes / (exp_yes + exp_no)
                scores.append(score)

        return scores

class SimpleLLM:
    """ç®€åŒ–çš„LLMåŒ…è£…å™¨ï¼Œä¸ä½¿ç”¨LlamaIndexçš„æŠ½è±¡åŸºç±»"""
    def __init__(self, model_path):

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.llm = LLM(model=model_path, gpu_memory_utilization=EnhancedConfig.gpu_memory_utilization, max_model_len=EnhancedConfig.max_model_len)
        self.sampling_params = SamplingParams(
            temperature=EnhancedConfig.TEMPERATURE,
            max_tokens=EnhancedConfig.MAX_NEW_TOKENS,
            top_p=EnhancedConfig.TOP_P)
    
    def chat(self, messages: List[ChatMessage]) -> ChatMessage:
        """èŠå¤©æ¥å£"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼šLlamaIndexçš„ChatMessage -> OpenAI APIæ ¼å¼
        openai_messages = []
        for msg in messages:
            openai_messages.append({
                "role": msg.role,
                "content": msg.content})

        prompts = self.tokenizer.apply_chat_template(openai_messages, 
                                                tokenize=False, 
                                                add_generation_prompt=True, 
                                                return_tensors="pt")
        response = self.llm.generate(prompts,self.sampling_params)

        return ChatMessage(
            role="assistant",
            content=response[0].outputs[0].text)

class EnhancedDocumentProcessor:
    """å¢å¼ºæ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šæ ¼å¼æ–‡æ¡£è¯»å–å’ŒçŸ¥è¯†å›¾è°±æ•°æ®"""
    
    def __init__(self):
        self.documents = []
        self.knowledge_processor = KnowledgeGraphProcessor()

    def read_all_documents(self, base_path: str, knowledge_graph_path: str = None, include_knowledge_graph: bool = True) -> List[Document]:
        """è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¯é€‰æ‹©æ˜¯å¦åŒ…å«çŸ¥è¯†å›¾è°±æ•°æ®"""
        all_docs = []
        
        # 1. è¯»å–ä¼ ç»Ÿæ–‡æ¡£
        print("=== è¯»å–ä¼ ç»Ÿæ–‡æ¡£ ===")
        traditional_docs = self._read_traditional_documents(base_path)
        all_docs.extend(traditional_docs)
        
        # 2. è¯»å–çŸ¥è¯†å›¾è°±æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if include_knowledge_graph and knowledge_graph_path and os.path.exists(knowledge_graph_path):
            print("\n=== è¯»å–çŸ¥è¯†å›¾è°±æ•°æ® ===")
            knowledge_docs = self.knowledge_processor.load_excel_knowledge_graph(knowledge_graph_path)
            all_docs.extend(knowledge_docs)
            
            research_docs = self.knowledge_processor.load_research_institutions(knowledge_graph_path)
            all_docs.extend(research_docs)
            print(f"âœ“ çŸ¥è¯†å›¾è°±æ•°æ®å·²åŠ è½½")
        elif include_knowledge_graph and knowledge_graph_path:
            print(f"\nâš  çŸ¥è¯†å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_graph_path}")
        else:
            print(f"\nâš  çŸ¥è¯†å›¾è°±æ•°æ®åŠ è½½å·²ç¦ç”¨")
        
        print(f"\næ€»è®¡è¯»å– {len(all_docs)} ä¸ªæ–‡æ¡£")
        print(f"  - ä¼ ç»Ÿæ–‡æ¡£: {len(traditional_docs)}")
        if include_knowledge_graph:
            print(f"  - çŸ¥è¯†å›¾è°±ä¼ä¸š: {len([d for d in all_docs if d.metadata.get('source_type') == 'knowledge_graph'])}")
            print(f"  - ç ”ç©¶æœºæ„: {len([d for d in all_docs if d.metadata.get('source_type') == 'research_institution'])}")
        
        return all_docs

    def _read_traditional_documents(self, base_path: str) -> List[Document]:
        """è¯»å–ä¼ ç»Ÿæ–‡æ¡£ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        folders = {
            "äººå·¥æ™ºèƒ½äº§ä¸šé“¾æ‹›å•†": "ai_industry",
            "æ‰¬å·å…¬ç§¯é‡‘æ”¿ç­–": "housing_fund", 
            "æ‰¬å·äººç¤¾å±€ç›¸å…³æ”¿ç­–": "hr_policy",
            "æ‰¬å·æ”¿åŠ¡æ”¿ç­–è§£è¯»": "government_policy"
        }

        all_docs = []
        doc_id = 0

        for folder_name, folder_type in folders.items():
            folder_path = os.path.join(base_path, folder_name)
            
            if not os.path.exists(folder_path):
                print(f"è­¦å‘Š: æ–‡ä»¶å¤¹ä¸å­˜åœ¨ {folder_path}")
                continue

            for file in os.listdir(folder_path):
                file_path = os.path.join(folder_path, file)
                try:
                    if file.endswith('.docx'):
                        content = self._read_docx(file_path)
                    elif file.endswith('.txt'):
                        content = self._read_txt(file_path)
                    else:
                        continue

                    if not content.strip():
                        continue

                    content = self._clean_text(content)
                    title = self._extract_title(content)

                    doc = Document(
                        text=content,
                        metadata={
                            'file_name': file,
                            'folder_type': folder_type,
                            'doc_id': doc_id,
                            'title': title,
                            'source_type': 'traditional_doc'
                        }
                    )
                    all_docs.append(doc)
                    doc_id += 1
                    print(f"  âœ“ {file} ({len(content)} å­—ç¬¦)")

                except Exception as e:
                    print(f"  âœ— {file}: {str(e)}")

        return all_docs

    # ä¿ç•™åŸæœ‰çš„è¾…åŠ©æ–¹æ³•
    def _read_docx(self, file_path: str) -> str:
        """è¯»å–docxæ ¼å¼æ–‡ä»¶å†…å®¹"""
        doc = docx.Document(file_path)
        paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]
        return '\n'.join(paragraphs)

    def _read_txt(self, file_path: str) -> str:
        """è¯»å–txtæ ¼å¼æ–‡ä»¶å†…å®¹"""
        encodings = ['utf-8', 'gbk', 'gb2312']
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except Exception:
                continue
        return ""

    def _clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        text = re.sub(r'\r\n|\r', '\n', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)
        return text.strip()

    def _extract_title(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–æ ‡é¢˜"""
        lines = text.split('\n')
        policy_keywords = ['é€šçŸ¥', 'æ„è§', 'åŠæ³•', 'æ–¹æ¡ˆ', 'æ¡ä¾‹', 'è§„å®š', 'æŒ‡å—', 'ç»†åˆ™']

        for line in lines[:5]:
            line = line.strip()
            if 5 < len(line) < 100:
                if any(keyword in line for keyword in policy_keywords):
                    return line
        
        return lines[0][:50].strip() if lines else "æœªå‘½åæ–‡æ¡£"

class Qwen3DirectReranker:
    """Qwen3 Rerankerç›´æ¥å°è£… - é€‚é…LlamaIndex NodeWithScoreæ ¼å¼"""
    def __init__(self, model_name: str = "Qwen/Qwen3-Reranker-8B", instruction: str = ""):
        self.model_name = model_name
        self.instruction = instruction
        self.reranker = None
        self._initialize_reranker()

    def _initialize_reranker(self):
        """åˆå§‹åŒ–Qwen3 Rerankerå®ä¾‹"""
        print(f"åˆå§‹åŒ–Qwen3 Reranker: {self.model_name}")
        self.reranker = Qwen3Reranker(self.model_name, self.instruction)
        print("Qwen3 Rerankeråˆå§‹åŒ–æˆåŠŸ")

    def rerank(self, query: str, nodes: List[NodeWithScore], top_n: int) -> List[NodeWithScore]:
        """å¯¹LlamaIndex NodeWithScoreåˆ—è¡¨è¿›è¡Œé‡æ’åºï¼Œè¿”å›Top-NèŠ‚ç‚¹"""
        # è¾¹ç•Œæ¡ä»¶å¤„ç†ï¼šrerankeræœªåˆå§‹åŒ–æˆ–èŠ‚ç‚¹ä¸ºç©ºæ—¶ï¼Œç›´æ¥è¿”å›å‰Nä¸ªèŠ‚ç‚¹
        if not self.reranker or not nodes:
            return nodes[:top_n]

        # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼ˆæå–èŠ‚ç‚¹æ–‡æœ¬ï¼‰
        pairs = [(query, node.node.text) for node in nodes]
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = self.reranker.compute_score(pairs)
        
        # æ›´æ–°èŠ‚ç‚¹åˆ†æ•°å¹¶æŒ‰åˆ†æ•°é™åºæ’åº
        for i, node in enumerate(nodes):
            node.score = float(scores[i])
        reranked_nodes = sorted(nodes, key=lambda x: x.score, reverse=True)
        return reranked_nodes[:top_n]

class SimpleVectorStore:
    """åŸºäºFAISSçš„å‘é‡å­˜å‚¨å®ç°"""
    def __init__(
        self, 
        nodes: List[BaseNode], 
        embed_model, 
        index_path: Optional[str] = None  # ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    ):
        self.nodes = nodes  # èŠ‚ç‚¹åˆ—è¡¨ï¼ˆéœ€ä¸ç´¢å¼•å‘é‡ä¸€ä¸€å¯¹åº”ï¼‰
        self.embed_model = embed_model  # åµŒå…¥æ¨¡å‹
        self.faiss_index = None
        self.index_path = index_path

        # ä¼˜å…ˆåŠ è½½å·²æœ‰ç´¢å¼•ï¼Œå¦åˆ™æ„å»ºæ–°ç´¢å¼•
        if index_path and os.path.exists(index_path):
            self._load_index()
        else:
            self._build_index()
            # è‹¥æŒ‡å®šäº†è·¯å¾„ï¼Œæ„å»ºåè‡ªåŠ¨ä¿å­˜
            if index_path:
                self.save_index()

    def _build_index(self):
        """æ„å»ºæ–°çš„FAISSç´¢å¼•"""
        print("æ„å»ºFAISSå‘é‡ç´¢å¼•...")
        batch_size = 8
        texts = [node.text for node in self.nodes]
        all_embeddings = []

        # æ‰¹é‡ç”ŸæˆåµŒå…¥
        for i in tqdm(range(0, len(texts), batch_size), desc="ç”ŸæˆèŠ‚ç‚¹åµŒå…¥"):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = self.embed_model.get_text_embeddings(batch_texts)
            all_embeddings.extend(batch_embeddings)

        embeddings_array = np.array(all_embeddings, dtype=np.float32)
        embedding_dim = embeddings_array.shape[1]
        faiss.normalize_L2(embeddings_array)

        # åˆå§‹åŒ–æ‰å¹³å†…ç§¯ç´¢å¼•ï¼ˆé€‚åˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.faiss_index = faiss.IndexFlatIP(embedding_dim)
        self.faiss_index.add(embeddings_array)
        print(f"âœ“ æ–°ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {self.faiss_index.ntotal} ä¸ªå‘é‡ï¼ˆç»´åº¦ï¼š{embedding_dim}ï¼‰")

    def save_index(self, path: Optional[str] = None):
        """ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°æ–‡ä»¶"""
        if not self.faiss_index:
            raise ValueError("ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„è·¯å¾„
        save_path = path or self.index_path
        if not save_path:
            raise ValueError("è¯·æŒ‡å®šç´¢å¼•ä¿å­˜è·¯å¾„")
        
        # åˆ›å»ºçˆ¶ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        faiss.write_index(self.faiss_index, save_path)
        print(f"âœ“ ç´¢å¼•å·²ä¿å­˜è‡³ï¼š{save_path}")

    def _load_index(self):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        if not self.index_path or not os.path.exists(self.index_path):
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.index_path}")
        
        self.faiss_index = faiss.read_index(self.index_path)
        # éªŒè¯ç´¢å¼•å‘é‡æ•°é‡ä¸èŠ‚ç‚¹æ•°é‡æ˜¯å¦åŒ¹é…
        if self.faiss_index.ntotal != len(self.nodes):
            raise ValueError(
                f"ç´¢å¼•å‘é‡æ•°é‡ï¼ˆ{self.faiss_index.ntotal}ï¼‰ä¸èŠ‚ç‚¹æ•°é‡ï¼ˆ{len(self.nodes)}ï¼‰ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯ç´¢å¼•æ–‡ä»¶ç‰ˆæœ¬ä¸ç¬¦"
            )
        print(f" å·²ä» {self.index_path} åŠ è½½ç´¢å¼•ï¼ŒåŒ…å« {self.faiss_index.ntotal} ä¸ªå‘é‡")

    def search(self, query: str, top_k: int = 10) -> List[NodeWithScore]:
        """æ£€ç´¢ç›¸ä¼¼èŠ‚ç‚¹"""
        if not self.faiss_index:
            raise ValueError("ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ£€ç´¢")
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥å¹¶å½’ä¸€åŒ–
        query_embedding = self.embed_model.get_query_embedding(query)
        query_array = np.array([query_embedding], dtype=np.float32)
        faiss.normalize_L2(query_array)

        # FAISSæ£€ç´¢ï¼ˆè¿”å›åˆ†æ•°å’Œç´¢å¼•ï¼‰
        similarities, top_indices = self.faiss_index.search(query_array, top_k)

        # å°è£…ç»“æœï¼ˆä»…ä¿ç•™æ­£åˆ†æ•°ï¼‰
        results = []
        for idx, score in zip(top_indices[0], similarities[0]):
            if score > 0:
                results.append(NodeWithScore(
                    node=self.nodes[idx],
                    score=float(score)
                ))
        return results

class EnhancedHybridRetriever:
    """å¢å¼ºç‰ˆæ··åˆæ£€ç´¢å™¨ - æ•´åˆæ‰€æœ‰ä¼˜åŒ–"""
    
    def __init__(
        self,
        vector_store,
        nodes: List[BaseNode],
        reranker=None,
        query_rewriter=None,
        query_decomposer=None,
        hypo_answer_generator=None,
        deduplicator=None,
        similarity_top_k: int = 100,
        bm25_top_k: int = 100,
        bm25_path: Optional[str] = None,
        bm25_enabled: bool = True
    ):
        self._vector_store = vector_store
        self._nodes = nodes
        self._reranker = reranker
        self._query_rewriter = query_rewriter
        self._query_decomposer = query_decomposer
        self._hypo_answer_generator = hypo_answer_generator
        self._deduplicator = deduplicator
        self._similarity_top_k = similarity_top_k
        self._bm25_top_k = bm25_top_k
        self._bm25_enabled = bm25_enabled
        self._bm25 = None
        self._bm25_path = bm25_path
        self._key_term_extractor = KeyTermExtractor()
        self._rrf = ReciprocalRankFusion()
        
        # åŠ è½½BM25ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._bm25_enabled:
            if bm25_path and os.path.exists(bm25_path):
                self._load_bm25_index()
            else:
                self._build_bm25_index()
                if bm25_path:
                    self._save_bm25_index()
        else:
            print("BM25æ£€ç´¢å·²ç¦ç”¨")

    def _build_bm25_index(self):
        """æ„å»ºBM25ç´¢å¼•"""
        if not self._bm25_enabled:
            return
            
        print("æ„å»ºBM25ç´¢å¼•...")
        corpus = [
            [token for token in jieba.cut(node.text) if len(token.strip()) > 1]
            for node in self._nodes
        ]
        self._bm25 = BM25Okapi(corpus)
        print(f"âœ“ BM25ç´¢å¼•æ„å»ºå®Œæˆï¼ˆ{len(corpus)}ä¸ªèŠ‚ç‚¹ï¼‰")

    def _save_bm25_index(self, path: Optional[str] = None):
        """ä¿å­˜BM25ç´¢å¼•"""
        if not self._bm25_enabled or not self._bm25:
            return
            
        import pickle
        save_path = path or self._bm25_path
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, "wb") as f:
            pickle.dump(self._bm25, f)
        print(f"âœ“ BM25ç´¢å¼•å·²ä¿å­˜è‡³ï¼š{save_path}")

    def _load_bm25_index(self, path: Optional[str] = None):
        """åŠ è½½BM25ç´¢å¼•"""
        if not self._bm25_enabled:
            return
            
        import pickle
        load_path = path or self._bm25_path
        with open(load_path, "rb") as f:
            self._bm25 = pickle.load(f)
        print(f"âœ“ å·²ä» {load_path} åŠ è½½BM25ç´¢å¼•")

    def _single_query_retrieve(self, query: str) -> List[NodeWithScore]:
        """å•æŸ¥è¯¢æ£€ç´¢"""
        # FAISSå‘é‡æ£€ç´¢
        vector_nodes = self._vector_store.search(query, self._similarity_top_k)
        vector_node_dict = {n.node.node_id: n.score for n in vector_nodes}

        # BM25æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        bm25_node_dict = {}
        if self._bm25_enabled and self._bm25:
            query_tokens = [token for token in jieba.cut(query) if len(token.strip()) > 1]
            bm25_scores = self._bm25.get_scores(query_tokens)
            bm25_top_indices = np.argsort(bm25_scores)[::-1][:self._bm25_top_k]
            bm25_node_dict = {
                self._nodes[idx].node_id: float(bm25_scores[idx])
                for idx in bm25_top_indices
                if bm25_scores[idx] > 0}

        # ç»“æœèåˆ
        all_node_ids = set(vector_node_dict.keys()) | set(bm25_node_dict.keys())
        merged_nodes = []

        # å¦‚æœæœ‰BM25ç»“æœï¼Œè¿›è¡Œèåˆï¼›å¦åˆ™åªä½¿ç”¨å‘é‡æ£€ç´¢ç»“æœ
        if bm25_node_dict:
            max_bm25_score = max(bm25_node_dict.values()) if bm25_node_dict else 1.0
            for node_id in all_node_ids:
                node = next((n for n in self._nodes if n.node_id == node_id), None)
                if not node:
                    continue

                vector_score = vector_node_dict.get(node_id, 0.0)
                bm25_score = bm25_node_dict.get(node_id, 0.0) / (max_bm25_score + 1e-6)
                combined_score = 0.5 * vector_score + 0.5 * bm25_score
                merged_nodes.append(NodeWithScore(node=node, score=combined_score))
        else:
            # åªä½¿ç”¨å‘é‡æ£€ç´¢
            for node_id, score in vector_node_dict.items():
                node = next((n for n in self._nodes if n.node_id == node_id), None)
                if node:
                    merged_nodes.append(NodeWithScore(node=node, score=score))

        merged_nodes.sort(key=lambda x: x.score, reverse=True)
        return merged_nodes

    def _pre_filter_nodes(self, query: str, nodes: List[NodeWithScore]) -> List[NodeWithScore]:
        """å…³é”®ä¿¡æ¯é¢„è¿‡æ»¤ - åŸºäºæ–‡æ¡£åç§°è®¡ç®—çš„æ™ºèƒ½è¯„åˆ†èåˆ"""
        key_terms = self._key_term_extractor.extract_key_terms(query)
        
        if not any(key_terms.values()):
            return nodes
        
        print(f"\n=== æ–‡æ¡£åç§°å…³é”®ä¿¡æ¯æ™ºèƒ½è¯„åˆ†èåˆ ===")
        print(f"æŸ¥è¯¢: {query}")
        print(f"æå–çš„å…³é”®ä¿¡æ¯: {key_terms}")

        filtered_nodes = []
        adjustment_stats = []
        
        for node in nodes:
            file_name = node.node.metadata.get('file_name', '')
            original_score = node.score

            # è®¡ç®—å…³é”®ä¿¡æ¯åŒ¹é…åº¦
            total_boost, detailed_scores = self._key_term_extractor.calculate_key_term_boost(
                file_name, key_terms, query
            )
            
            # æ™ºèƒ½è¯„åˆ†èåˆç­–ç•¥
            new_score = self._intelligent_score_fusion(
                original_score, total_boost, detailed_scores, file_name
            )
            
            node.score = new_score
            filtered_nodes.append(node)
            
            # è®°å½•è°ƒæ•´ç»Ÿè®¡
            adjustment_stats.append({
                'file_name': file_name,
                'original_score': original_score,
                'new_score': new_score,
                'total_boost': total_boost,
                'adjustment': new_score - original_score
            })
            
            # æ‰“å°è¯¦ç»†è°ƒæ•´ä¿¡æ¯
            self._print_adjustment_details(
                file_name, original_score, new_score, total_boost, detailed_scores
            )
        
        # é‡æ–°æ’åºå¹¶è¾“å‡ºç»Ÿè®¡
        filtered_nodes.sort(key=lambda x: x.score, reverse=True)
        self._print_fusion_statistics(adjustment_stats, filtered_nodes)
        
        return filtered_nodes

    def _intelligent_score_fusion(self, original_score: float, total_boost: float, 
                                detailed_scores: Dict[str, float], file_name: str) -> float:
        """æ™ºèƒ½è¯„åˆ†èåˆç­–ç•¥"""
        
        # åŸºç¡€è°ƒæ•´ï¼šä½¿ç”¨sigmoidå‡½æ•°è¿›è¡Œå¹³æ»‘è°ƒæ•´
        def sigmoid_adjustment(x):
            return 4 / (1 + math.exp(-2 * x)) - 2  # è¾“å‡ºèŒƒå›´[-2, 2]
        
        # 1. å¯¹äºé«˜åŸå§‹åˆ†æ•°ï¼Œè°ƒæ•´è¦æ›´è°¨æ…
        if original_score > 1.5:
            adjustment_factor = 0.3  # é«˜åˆ†æ•°æ–‡æ¡£è°ƒæ•´å¹…åº¦è¾ƒå°
        elif original_score > 0.8:
            adjustment_factor = 0.6  # ä¸­ç­‰åˆ†æ•°æ–‡æ¡£è°ƒæ•´å¹…åº¦ä¸­ç­‰
        else:
            adjustment_factor = 0.8  # ä½åˆ†æ•°æ–‡æ¡£è°ƒæ•´å¹…åº¦è¾ƒå¤§

        # 2. ä½¿ç”¨sigmoidå‡½æ•°å¹³æ»‘è°ƒæ•´å€¼
        smoothed_boost = sigmoid_adjustment(total_boost) * adjustment_factor
        
        # 3. è€ƒè™‘å„é¡¹å¾—åˆ†çš„æƒé‡åˆ†å¸ƒ
        positive_components = sum([
            detailed_scores['semantic_similarity'],
            detailed_scores['year_match'],
            detailed_scores['policy_match'],
            detailed_scores['location_match'],
            detailed_scores['phrase_match'],
            detailed_scores['time_range_match']
        ])
        
        negative_components = sum([
            detailed_scores['location_penalty'],
            detailed_scores['time_penalty']
        ])
        
        # 4. è®¡ç®—ç½®ä¿¡åº¦æƒé‡
        confidence_weight = min(1.0, positive_components / 3.0)  # æ­£é¡¹å¾—åˆ†è¶Šé«˜ï¼Œç½®ä¿¡åº¦è¶Šé«˜
        
        # 5. æœ€ç»ˆè°ƒæ•´è®¡ç®—
        if total_boost > 0:
            # æ­£å‘è°ƒæ•´ï¼šåŸºäºç½®ä¿¡åº¦åŠ æƒ
            final_adjustment = smoothed_boost * confidence_weight
            new_score = original_score * (1 + final_adjustment)
        else:
            # è´Ÿå‘è°ƒæ•´ï¼šæ›´è°¨æ…ï¼Œè€ƒè™‘åŸå§‹åˆ†æ•°
            penalty_severity = 0.5 if original_score > 0.6 else 0.3
            final_adjustment = smoothed_boost * penalty_severity
            new_score = original_score * (1 + final_adjustment)
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†… [0, 1]
        new_score = max(0.0, min(1.0, new_score))
        
        return new_score

    def _print_adjustment_details(self, file_name: str, original_score: float, 
                                new_score: float, total_boost: float, 
                                detailed_scores: Dict[str, float]):
        """æ‰“å°è°ƒæ•´è¯¦æƒ…"""
        print(f"\nğŸ“„ æ–‡æ¡£: {file_name}")
        print(f"   åŸå§‹åˆ†æ•°: {original_score:.3f} â†’ æ–°åˆ†æ•°: {new_score:.3f}")
        print(f"   æ€»è°ƒæ•´å€¼: {total_boost:.2f}")
        
        # æ‰“å°æ­£é¡¹å¾—åˆ†
        positive_scores = {k: v for k, v in detailed_scores.items() if v > 0 and 'penalty' not in k}
        if positive_scores:
            print("   âœ… å¥–åŠ±é¡¹:")
            for key, value in positive_scores.items():
                if value > 0:
                    print(f"     - {key}: +{value:.2f}")
        
        # æ‰“å°è´Ÿé¡¹å¾—åˆ†
        negative_scores = {k: v for k, v in detailed_scores.items() if v > 0 and 'penalty' in k}
        if negative_scores:
            print("   âŒ æƒ©ç½šé¡¹:")
            for key, value in negative_scores.items():
                if value > 0:
                    print(f"     - {key}: -{value:.2f}")

    def _print_fusion_statistics(self, adjustment_stats: List[dict], filtered_nodes: List[NodeWithScore]):
        """æ‰“å°èåˆç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n=== æ™ºèƒ½è¯„åˆ†èåˆç»Ÿè®¡ ===")
        print(f"å¤„ç†èŠ‚ç‚¹æ€»æ•°: {len(adjustment_stats)}")
        
        if adjustment_stats:
            avg_original = sum(stat['original_score'] for stat in adjustment_stats) / len(adjustment_stats)
            avg_new = sum(stat['new_score'] for stat in adjustment_stats) / len(adjustment_stats)
            avg_adjustment = sum(stat['adjustment'] for stat in adjustment_stats) / len(adjustment_stats)
            
            print(f"å¹³å‡åŸå§‹åˆ†æ•°: {avg_original:.3f}")
            print(f"å¹³å‡æ–°åˆ†æ•°: {avg_new:.3f}")
            print(f"å¹³å‡è°ƒæ•´å¹…åº¦: {avg_adjustment:+.3f}")
            
            # è°ƒæ•´å¹…åº¦åˆ†å¸ƒ
            positive_adjustments = [stat for stat in adjustment_stats if stat['adjustment'] > 0]
            negative_adjustments = [stat for stat in adjustment_stats if stat['adjustment'] < 0]
            no_adjustments = [stat for stat in adjustment_stats if stat['adjustment'] == 0]
            
            print(f"åˆ†æ•°æå‡æ–‡æ¡£: {len(positive_adjustments)}ä¸ª")
            print(f"åˆ†æ•°é™ä½æ–‡æ¡£: {len(negative_adjustments)}ä¸ª")
            print(f"åˆ†æ•°ä¸å˜æ–‡æ¡£: {len(no_adjustments)}ä¸ª")
        
        if filtered_nodes:
            print(f"è°ƒæ•´åæœ€é«˜åˆ†: {filtered_nodes[0].score:.3f}")
            print(f"è°ƒæ•´åæœ€ä½åˆ†: {filtered_nodes[-1].score:.3f}")

    def retrieve(self, original_query: str) -> List[NodeWithScore]:
        """å¢å¼ºæ£€ç´¢ä¸»æµç¨‹"""
        all_queries = [original_query]

        # 1. æŸ¥è¯¢æ”¹å†™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._query_rewriter and EnhancedConfig.QUERY_REWRITE_ENABLED:
            rewritten_queries = self._query_rewriter.rewrite_queries(
                original_query, EnhancedConfig.QUERY_REWRITE_NUM
            )
            all_queries.extend(rewritten_queries)
        
        # 2. æŸ¥è¯¢åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        sub_queries = []
        if self._query_decomposer and EnhancedConfig.QUERY_DECOMPOSE_ENABLED:
            sub_queries = self._query_decomposer.decompose_query(original_query)
            all_queries.extend(sub_queries)

        # 3. å‡è®¾ç­”æ¡ˆæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._hypo_answer_generator and EnhancedConfig.HYPO_ANSWER_ENABLED:
            hypo_answer = self._hypo_answer_generator.generate_hypothetical_answer(original_query)
            all_queries.append(hypo_answer)
        
        # å»é‡æŸ¥è¯¢
        all_queries = list(set(all_queries))
        print(f"æ‰§è¡Œå¤šæŸ¥è¯¢æ£€ç´¢: {len(all_queries)}ä¸ªæŸ¥è¯¢")
        print('æŸ¥è¯¢:',all_queries)
        
        # 4. å¹¶è¡Œæ‰§è¡Œå¤šæŸ¥è¯¢æ£€ç´¢
        all_ranked_lists = []
        with ThreadPoolExecutor() as executor:
            future_to_query = {
                executor.submit(self._single_query_retrieve, query): query 
                for query in all_queries
            }

            for future in as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    results = future.result()
                    # å…³é”®ä¿¡æ¯è¿‡æ»¤
                    filtered_results = self._pre_filter_nodes(query, results)
                    all_ranked_lists.append(filtered_results)
                except Exception as e:
                    print(f"æŸ¥è¯¢ '{query}' æ£€ç´¢å¤±è´¥: {e}")

        # 5. RRFèåˆ
        if self._rrf and EnhancedConfig.RRF_ENABLED:
            
            if len(all_ranked_lists) > 1:
                fused_nodes = self._rrf.fuse(all_ranked_lists)
            else:
                fused_nodes = all_ranked_lists[0] if all_ranked_lists else []
        
        # 5.5 åº”ç”¨å…³é”®ä¿¡æ¯è°ƒæ•´
        # fused_nodes = KeyTermExtractor.apply_key_term_adjustment(fused_nodes, original_query)
        
        # 6. å»é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._deduplicator and EnhancedConfig.DEDUPLICATE_ENABLED:
            deduplicated_nodes = self._deduplicator.deduplicate_nodes(fused_nodes)
        else:
            deduplicated_nodes = fused_nodes

        # 7. é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._reranker and EnhancedConfig.RERANKER_ENABLED and deduplicated_nodes:
            reranked_nodes = self._reranker.rerank(
                original_query,
                deduplicated_nodes[:EnhancedConfig.RERANK_TOP_N],
                EnhancedConfig.RERANK_TOP_N
            )
            final_nodes = reranked_nodes + deduplicated_nodes[EnhancedConfig.RERANK_TOP_N:]
        else:
            final_nodes = deduplicated_nodes
        
        return final_nodes[:EnhancedConfig.FINAL_TOP_K]

class FullyOptimizedQASystem:
    """å®Œå…¨ä¼˜åŒ–çš„é—®ç­”ç³»ç»Ÿ - æ”¯æŒçŸ¥è¯†å›¾è°±æ•°æ®"""
    
    def __init__(self):
        self.vector_store = None
        self.retriever = None
        self.llm = None
        self.embed_model = None
        self.nodes = []
        self.doc_chunk_map = {}
        self.faiss_path = '../data/output/faiss_index.bin'
        self.bm25_path = '../data/output/bm25_index.pkl'
        self.knowledge_graph_path = '../data/æ‰¬å·å¸‚äººå·¥æ™ºèƒ½äº§ä¸šå›¾è°±.xlsx'
        self.include_knowledge_graph = None

        # ç»„ä»¶å®ä¾‹
        self.query_rewriter = None
        self.query_decomposer = None
        self.hypo_answer_generator = None
        self.reranker = None
        self.deduplicator = None
        self.splitter = None

    def initialize(self, doc_path: str, include_knowledge_graph: bool = True):
        """åˆå§‹åŒ–ç³»ç»Ÿ - å¯é€‰æ‹©æ˜¯å¦åŒ…å«çŸ¥è¯†å›¾è°±æ•°æ®
        
        Args:
            doc_path: ä¼ ç»Ÿæ–‡æ¡£è·¯å¾„
            include_knowledge_graph: æ˜¯å¦åŒ…å«çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œé»˜è®¤ä¸ºTrue
        """
        self.include_knowledge_graph = EnhancedConfig.include_knowledge_graph
        
        # 1. åŠ è½½æ¨¡å‹
        print("\n1. åŠ è½½æ¨¡å‹...")
        self.embed_model = Qwen3Embedding(
            model_name=EnhancedConfig.EMBEDDING_MODEL,
            instruction=EnhancedConfig.EMBEDDING_INSTRUCTION)

        # 2. å¤„ç†æ–‡æ¡£ï¼ˆå¯é€‰æ‹©æ˜¯å¦åŒ…å«çŸ¥è¯†å›¾è°±ï¼‰
        print("\n2. è¯»å–å’Œå¤„ç†æ–‡æ¡£...")
        processor = EnhancedDocumentProcessor()
        documents = processor.read_all_documents(
            doc_path, 
            knowledge_graph_path=self.knowledge_graph_path,
            include_knowledge_graph=self.include_knowledge_graph
        )

        # 3. æ–‡æ¡£åˆ†å‰²
        print(f"\n3. æ–‡æ¡£åˆ†å‰² - ä½¿ç”¨ {EnhancedConfig.CHUNK_MODE} æ¨¡å¼...")
        self.splitter = DocumentSplitterFactory.create_splitter()
        
        all_chunks = []
        for doc in documents:
            chunks = self.splitter.split_document(doc)
            all_chunks.extend(chunks)
        
        self.nodes = all_chunks
        print(f" ç”Ÿæˆ {len(self.nodes)} ä¸ª{EnhancedConfig.CHUNK_MODE}èŠ‚ç‚¹")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹æ–‡æ¡£çš„æ•°é‡
        source_types = {}
        for node in self.nodes:
            source_type = node.metadata.get('source_type', 'unknown')
            source_types[source_type] = source_types.get(source_type, 0) + 1
        
        print("æ–‡æ¡£ç±»å‹ç»Ÿè®¡:")
        for source_type, count in source_types.items():
            print(f"  - {source_type}: {count}")

        # æ„å»ºæ–‡æ¡£-æ®µè½æ˜ å°„
        for node in self.nodes:
            file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
            if file_name not in self.doc_chunk_map:
                self.doc_chunk_map[file_name] = []
            self.doc_chunk_map[file_name].append(node)

        # 4. æ„å»ºç´¢å¼•
        print("\n4. æ„å»ºå‘é‡ç´¢å¼•...")
        self.vector_store = SimpleVectorStore(
            nodes=self.nodes,
            embed_model=self.embed_model,
            index_path=self.faiss_path
        )

        # 5. åŠ è½½é‡æ’åºå™¨
        if EnhancedConfig.RERANKER_ENABLED:
            print("\n5. åŠ è½½é‡æ’åºå™¨...")
            self.reranker = Qwen3DirectReranker(
                model_name=EnhancedConfig.RERANKER_MODEL,
                instruction=EnhancedConfig.RERANKER_INSTRUCTION)
        else:
            print("\n5. é‡æ’åºå™¨å·²ç¦ç”¨")
            self.reranker = None
        
        # 6. åŠ è½½LLMå’Œç»„ä»¶
        print("\n6. åŠ è½½LLMå’Œç»„ä»¶...")
        self.llm = SimpleLLM(model_path=EnhancedConfig.GENERATION_MODEL)
        
        # åˆå§‹åŒ–å¯é€‰ç»„ä»¶
        if EnhancedConfig.QUERY_REWRITE_ENABLED:
            self.query_rewriter = QueryRewriter(self.llm)
            print("  - æŸ¥è¯¢æ”¹å†™å™¨å·²å¯ç”¨")
        else:
            self.query_rewriter = None
            print("  - æŸ¥è¯¢æ”¹å†™å™¨å·²ç¦ç”¨")
            
        if EnhancedConfig.QUERY_DECOMPOSE_ENABLED:
            self.query_decomposer = QueryDecomposer(self.llm)
            print("  - æŸ¥è¯¢åˆ†è§£å™¨å·²å¯ç”¨")
        else:
            self.query_decomposer = None
            print("  - æŸ¥è¯¢åˆ†è§£å™¨å·²ç¦ç”¨")
            
        if EnhancedConfig.HYPO_ANSWER_ENABLED:
            self.hypo_answer_generator = HypotheticalAnswerGenerator(self.llm)
            print("  - å‡è®¾ç­”æ¡ˆç”Ÿæˆå™¨å·²å¯ç”¨")
        else:
            self.hypo_answer_generator = None
            print("  - å‡è®¾ç­”æ¡ˆç”Ÿæˆå™¨å·²ç¦ç”¨")
            
        if EnhancedConfig.DEDUPLICATE_ENABLED:
            self.deduplicator = SimHashDeduplicator(EnhancedConfig)
            print("  - å»é‡å™¨å·²å¯ç”¨")
        else:
            self.deduplicator = None
            print("  - å»é‡å™¨å·²ç¦ç”¨")

        # 7. åˆ›å»ºå¢å¼ºæ£€ç´¢å™¨
        print("\n7. åˆå§‹åŒ–å¢å¼ºæ£€ç´¢å™¨...")
        self.retriever = EnhancedHybridRetriever(
            vector_store=self.vector_store,
            nodes=self.nodes,
            reranker=self.reranker,
            query_rewriter=self.query_rewriter,
            query_decomposer=self.query_decomposer,
            hypo_answer_generator=self.hypo_answer_generator,
            deduplicator=self.deduplicator,
            bm25_path=self.bm25_path,
            bm25_enabled=EnhancedConfig.BM25_ENABLED
        )

        print("\nâœ“ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print(f"  çŸ¥è¯†å›¾è°±é›†æˆ: {'å·²åŠ è½½' if EnhancedConfig.include_knowledge_graph else 'å·²ç¦ç”¨'}")
        print(f"  æ€»æ–‡æ¡£èŠ‚ç‚¹: {len(self.nodes)}")
        print(f"  ç»„ä»¶çŠ¶æ€:")
        print(f"  - æŸ¥è¯¢æ”¹å†™: {'å¯ç”¨' if EnhancedConfig.QUERY_REWRITE_ENABLED else 'ç¦ç”¨'}")
        print(f"  - æŸ¥è¯¢åˆ†è§£: {'å¯ç”¨' if EnhancedConfig.QUERY_DECOMPOSE_ENABLED else 'ç¦ç”¨'}")
        print(f"  - å‡è®¾ç­”æ¡ˆ: {'å¯ç”¨' if EnhancedConfig.HYPO_ANSWER_ENABLED else 'ç¦ç”¨'}")
        print(f"  - å»é‡: {'å¯ç”¨' if EnhancedConfig.DEDUPLICATE_ENABLED else 'ç¦ç”¨'}")
        print(f"  - é‡æ’åº: {'å¯ç”¨' if EnhancedConfig.RERANKER_ENABLED else 'ç¦ç”¨'}")
        print(f"  - BM25: {'å¯ç”¨' if EnhancedConfig.BM25_ENABLED else 'ç¦ç”¨'}")

    def get_adjacent_chunks(self, current_node, n=1):
        """è·å–ç›¸é‚»å—"""
        file_name = current_node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
        
        if file_name not in self.doc_chunk_map:
            return [], []
        
        chunks = self.doc_chunk_map[file_name]
        
        # æ ¹æ®åˆ†å‰²æ¨¡å¼é€‰æ‹©IDå­—æ®µ
        if EnhancedConfig.CHUNK_MODE == 'paragraph':
            current_id = current_node.metadata.get('paragraph_id', -1)
        else:
            current_id = current_node.metadata.get('chunk_id', -1)
            
        if current_id < 0:
            return [], []
        
        total_chunks = len(chunks)
        
        prev_chunks = []
        next_chunks = []
        
        # å‰åºå—
        start_prev = max(0, current_id - n)
        for idx in range(current_id - 1, start_prev - 1, -1):
            prev_chunks.append(chunks[idx])

        # ååºå—
        end_next = min(total_chunks - 1, current_id + n)
        for idx in range(current_id + 1, end_next + 1):
            next_chunks.append(chunks[idx])
        
        return prev_chunks, next_chunks

    def _build_context_str(self, retrieved_nodes, n=1):
        """æ„å»ºä¸Šä¸‹æ–‡ - é›†æˆSimHashå»é‡ç¡®ä¿æ— é‡å¤æ®µè½"""
        context_parts = []
        total_length = 0
        max_context_length = EnhancedConfig.max_context_length
        
        # å»é‡ç›¸å…³é›†åˆ
        added_chunk_ids = set()
        added_content_hashes = set()  # ç”¨äºå­˜å‚¨å·²æ·»åŠ å†…å®¹çš„SimHash
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_retrieved = len(retrieved_nodes)
        low_score_skipped = 0
        duplicate_skipped = 0
        simhash_duplicate_skipped = 0
        added_count = 0
        threshold = 0.2
        
        print(f"\n=== ä¸Šä¸‹æ–‡æ„å»ºç»Ÿè®¡ ===")
        print(f"æ£€ç´¢åˆ°çš„èŠ‚ç‚¹æ€»æ•°: {total_retrieved}")
        print(f"åˆ†æ•°é˜ˆå€¼: {threshold}")
        print(f"ç›¸é‚»å—æ•°é‡: {n}")
        print(f"æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {max_context_length}")
        print(f"SimHashç›¸ä¼¼é˜ˆå€¼: {EnhancedConfig.SIMILAR_THRESHOLD}")
        
        # é¦–å…ˆå¯¹æ£€ç´¢åˆ°çš„èŠ‚ç‚¹æŒ‰åˆ†æ•°æ’åºï¼ˆé™åºï¼‰
        sorted_nodes = sorted(retrieved_nodes, key=lambda x: x.score, reverse=True)
        
        for node_with_score in sorted_nodes:
            if node_with_score.score <= threshold:
                low_score_skipped += 1
                continue

            node = node_with_score.node
            file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')

            # æ ¹æ®åˆ†å‰²æ¨¡å¼é€‰æ‹©IDå­—æ®µ
            if EnhancedConfig.CHUNK_MODE == 'paragraph':
                chunk_id = node.metadata.get('paragraph_id', 'æœªçŸ¥')
            else:
                chunk_id = node.metadata.get('chunk_id', 'æœªçŸ¥')

            chunk_key = f"{file_name}_{chunk_id}"
            
            if chunk_key in added_chunk_ids:
                duplicate_skipped += 1
                continue

            # è·å–ç›¸é‚»å—
            prev_chunks, next_chunks = self.get_adjacent_chunks(node, n)

            # æ„å»ºå½“å‰èŠ‚ç‚¹åŠå…¶ç›¸é‚»å—çš„å†…å®¹
            chunks_to_add = []
            
            # æ·»åŠ å‰åºç›¸é‚»å—
            for prev_chunk in prev_chunks:
                if EnhancedConfig.CHUNK_MODE == 'paragraph':
                    prev_chunk_id = prev_chunk.metadata.get('paragraph_id', 'æœªçŸ¥')
                else:
                    prev_chunk_id = prev_chunk.metadata.get('chunk_id', 'æœªçŸ¥')
                prev_key = f"{file_name}_{prev_chunk_id}"
                if prev_key not in added_chunk_ids:
                    chunks_to_add.append((prev_chunk, prev_key, f"å‰åº-{prev_chunk_id}"))
            
            # æ·»åŠ å½“å‰å—
            chunks_to_add.append((node, chunk_key, f"å½“å‰-{chunk_id}"))
            
            # æ·»åŠ åç»­ç›¸é‚»å—
            for next_chunk in next_chunks:
                if EnhancedConfig.CHUNK_MODE == 'paragraph':
                    next_chunk_id = next_chunk.metadata.get('paragraph_id', 'æœªçŸ¥')
                else:
                    next_chunk_id = next_chunk.metadata.get('chunk_id', 'æœªçŸ¥')
                next_key = f"{file_name}_{next_chunk_id}"
                if next_key not in added_chunk_ids:
                    chunks_to_add.append((next_chunk, next_key, f"åç»­-{next_chunk_id}"))

            # å¤„ç†æ‰€æœ‰è¦æ·»åŠ çš„å—ï¼Œè¿›è¡ŒSimHashå»é‡
            for chunk, chunk_key, chunk_type in chunks_to_add:
                if chunk_key in added_chunk_ids:
                    continue
                    
                chunk_text = chunk.text.strip()
                if len(chunk_text) < EnhancedConfig.MIN_TEXT_LENGTH:
                    # å¯¹äºè¿‡çŸ­çš„æ–‡æœ¬ï¼Œç›´æ¥æ·»åŠ 
                    chunk_info = f"ã€{chunk_type}ã€‘å¾—åˆ†ï¼š{node_with_score.score:.2f} æ–‡æ¡£: {file_name}\n{chunk_text}\n\n"
                    
                    if total_length + len(chunk_info) > max_context_length:
                        break
                    
                    context_parts.append(chunk_info)
                    total_length += len(chunk_info)
                    added_chunk_ids.add(chunk_key)
                    added_count += 1
                    print(f"æ·»åŠ çŸ­æ–‡æœ¬èŠ‚ç‚¹: {file_name} ({chunk_type}), åˆ†æ•°: {node_with_score.score:.3f}, å½“å‰æ€»é•¿åº¦: {total_length}")
                    continue
                
                # è®¡ç®—å½“å‰å—çš„SimHash
                chunk_hash = self.deduplicator._simhash(chunk_text)
                
                # æ£€æŸ¥æ˜¯å¦ä¸å·²æ·»åŠ å†…å®¹é‡å¤
                is_duplicate = False
                for existing_hash in added_content_hashes:
                    # print('æ–‡æ¡£è·ç¦»ï¼š', self.deduplicator._hamming_distance(chunk_hash, existing_hash))
                    
                    if self.deduplicator._hamming_distance(chunk_hash, existing_hash) <= EnhancedConfig.SIMILAR_THRESHOLD:
                        
                        is_duplicate = True
                        simhash_duplicate_skipped += 1
                        print(f"SimHashå»é‡: {file_name} ({chunk_type}), æ£€æµ‹åˆ°é‡å¤å†…å®¹")
                        break
                
                if not is_duplicate:
                    chunk_info = f"ã€{chunk_type}ã€‘å¾—åˆ†ï¼š{node_with_score.score:.2f} æ–‡æ¡£: {file_name}\n{chunk_text}\n\n"
                    
                    if total_length + len(chunk_info) > max_context_length:
                        print(f"è¾¾åˆ°æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œåœæ­¢æ·»åŠ æ›´å¤šå†…å®¹")
                        break
                    
                    context_parts.append(chunk_info)
                    total_length += len(chunk_info)
                    added_chunk_ids.add(chunk_key)
                    added_content_hashes.add(chunk_hash)
                    added_count += 1
                    print(f"æ·»åŠ èŠ‚ç‚¹: {file_name} ({chunk_type}), åˆ†æ•°: {node_with_score.score:.3f}, å½“å‰æ€»é•¿åº¦: {total_length}")
            
            if total_length >= max_context_length:
                print(f"è¾¾åˆ°æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œåœæ­¢å¤„ç†æ›´å¤šèŠ‚ç‚¹")
                break
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n=== å»é‡ç»Ÿè®¡ç»“æœ ===")
        print(f"æ€»æ£€ç´¢èŠ‚ç‚¹æ•°: {total_retrieved}")
        print(f"ä½åˆ†è·³è¿‡: {low_score_skipped}")
        print(f"é‡å¤IDè·³è¿‡: {duplicate_skipped}")
        print(f"SimHashé‡å¤è·³è¿‡: {simhash_duplicate_skipped}")
        print(f"æˆåŠŸæ·»åŠ : {added_count}")
        print(f"æœ€ç»ˆä¸Šä¸‹æ–‡é•¿åº¦: {total_length}")
        
        return "".join(context_parts)

    def build_prompt(self, question: str, context_str: str) -> str:
        """æ„å»ºæç¤ºè¯ - æ”¯æŒçŸ¥è¯†å›¾è°±æ•°æ®"""
        return f"""
# è§’è‰²å®šä½
æ‚¨æ˜¯ä¸€ä½èµ„æ·±çš„æ‰¬å·å¸‚æ”¿ç­–å’¨è¯¢ä¸äººå·¥æ™ºèƒ½äº§ä¸šé“¾æ‹›å•†ä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„æ”¿ç­–è§£è¯»èƒ½åŠ›å’Œäº§ä¸šåˆ†æç»éªŒã€‚

# ä»»åŠ¡æŒ‡ä»¤
è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ”¿ç­–æ–‡æ¡£å’Œäº§ä¸šå›¾è°±æ•°æ®ï¼Œå¯¹ç”¨æˆ·é—®é¢˜è¿›è¡Œè¯¦ç»†ã€å‡†ç¡®ã€å…¨é¢çš„è§£ç­”ã€‚

# å¯ç”¨æ•°æ®æº
{context_str}

# å¾…å›ç­”é—®é¢˜
{question}

# å›ç­”è¦æ±‚
## å‡†ç¡®æ€§è¦æ±‚
- ä¸¥æ ¼ä¾æ®æä¾›çš„æ”¿ç­–æ–‡æ¡£å’Œäº§ä¸šå›¾è°±æ•°æ®ï¼Œæ‰€æœ‰ä¿¡æ¯å¿…é¡»ä¸åŸæ–‡ä¿æŒä¸€è‡´
- å…³é”®ä¿¡æ¯ï¼ˆæ•°å­—ã€æ—¥æœŸã€æ”¿ç­–æ¡æ¬¾ã€ä¼ä¸šåç§°ã€ä¸šåŠ¡èŒƒå›´ã€åœ°åŒºç­‰ï¼‰å¿…é¡»å®Œæ•´å¤åˆ»åŸæ–‡
- ä¸¥ç¦ä»»ä½•å½¢å¼çš„æœæ’°ã€æ”¹å†™ã€æ¦‚æ‹¬æˆ–ä¸»è§‚æ¨æ–­

## å†…å®¹æ·±åº¦è¦æ±‚
- å¯¹æ”¿ç­–æ¡æ¬¾è¿›è¡Œå®Œæ•´è§£è¯»ï¼Œè¯´æ˜æ”¿ç­–é€‚ç”¨èŒƒå›´ã€æ¡ä»¶ã€æ ‡å‡†ç­‰å…³é”®è¦ç´ 
- å¯¹äº§ä¸šä¿¡æ¯è¿›è¡Œå…¨é¢åˆ†æï¼ŒåŒ…æ‹¬ä¼ä¸šåˆ†å¸ƒã€ä¸šåŠ¡ç‰¹ç‚¹ã€åŒºåŸŸç‰¹å¾ç­‰
- é’ˆå¯¹åˆ—ä¸¾ç±»é—®é¢˜ï¼Œå¿…é¡»å®Œæ•´å‘ˆç°æ‰€æœ‰ç›¸å…³æ¡ç›®ï¼Œä¸å¾—é—æ¼ä»»ä½•é‡è¦ä¿¡æ¯
- å¯¹äºå¤æ‚é—®é¢˜ï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼Œç¡®ä¿å›ç­”çš„å…¨é¢æ€§

## ç»“æ„åŒ–è¦æ±‚
- å›ç­”å†…å®¹åº”è¯¥å±‚æ¬¡æ¸…æ™°ï¼Œé€»è¾‘ä¸¥è°¨
- é‡è¦ä¿¡æ¯å¯ä»¥é€‚å½“ä½¿ç”¨åˆ†ç‚¹ï¼ˆ1.2.3...ï¼‰è¯´æ˜ï¼Œä½†é¿å…ä½¿ç”¨ç‰¹æ®Šç¬¦å·
- ä¿æŒè¯­å¥çš„è‡ªç„¶æµç•…ï¼ŒåŒæ—¶ç¡®ä¿ä¿¡æ¯çš„å®Œæ•´ä¼ è¾¾

## ä¸“ä¸šæ€§è¦æ±‚
- ä½¿ç”¨æ”¿ç­–æ–‡æ¡£å’Œäº§ä¸šå›¾è°±ä¸­çš„ä¸“ä¸šæœ¯è¯­å’Œæ ¸å¿ƒæ¦‚å¿µ
- ä¿æŒæ”¿ç­–è¯­è¨€çš„ä¸¥è°¨æ€§å’Œäº§ä¸šåˆ†æçš„æ·±åº¦
- åœ¨å‡†ç¡®çš„åŸºç¡€ä¸Šï¼Œç¡®ä¿å›ç­”æ˜“äºç†è§£
- ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸ç”¨å¤è¿°é¢˜ç›®

## å®Œæ•´æ€§è¦æ±‚
- å›ç­”å¿…é¡»å®Œå…¨è¦†ç›–ç”¨æˆ·é—®é¢˜çš„æ‰€æœ‰æ–¹é¢
- å¯¹äºæ¶‰åŠå¤šä¸ªæ”¿ç­–æˆ–å¤šä¸ªä¼ä¸šçš„é—®é¢˜ï¼Œéœ€è¦æ•´åˆæ‰€æœ‰ç›¸å…³ä¿¡æ¯
- ç¡®ä¿å›ç­”èƒ½å¤Ÿå®Œæ•´åœ°è§£å†³ç”¨æˆ·é—®é¢˜

è¯·åŸºäºä¸Šè¿°è¦æ±‚ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚
"""

    def answer(self, question: str) -> str:
        """é—®ç­”æ¥å£"""
        if not self.retriever:
            return "ç³»ç»Ÿæœªåˆå§‹åŒ–"
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_nodes = self.retriever.retrieve(question)
        if not retrieved_nodes:
            return "æœªæ‰¾åˆ°ç›¸å…³æ”¿ç­–ä¿¡æ¯ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_str = self._build_context_str(retrieved_nodes, n=1)
        print(context_str)
        # ç”Ÿæˆç­”æ¡ˆ
        prompt = self.build_prompt(question, context_str)
        messages = [ChatMessage(role="user", content=prompt)]
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.llm.chat(messages)
                answer = response.content
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"
                time.sleep(2)
        
        # åå¤„ç†
        return self._post_process(answer)

    def _post_process(self, answer: str) -> str:
        """ç­”æ¡ˆåå¤„ç†"""
        # æ¸…ç†æ ¼å¼
        pattern = r'(.*?)</think>(.*)'
        match = re.search(pattern, answer, re.DOTALL)
        
        if match:
            content = match.group(2).strip()
        else:
            content = answer.strip()
        
        final_answer = re.sub(r'\n+', '\n', content).strip()
        return final_answer