import os
import re
import json
import uuid
import pickle
import warnings
import requests # æ–°å¢ï¼šç”¨äº Reranker API è°ƒç”¨
from typing import List, Dict, Any, Union
from dataclasses import dataclass

import numpy as np
import torch
import faiss
import jieba
import jieba.analyse
import docx
from rank_bm25 import BM25Okapi

# æœ¬åœ°æ¨¡å‹ç›¸å…³åº“ (ä»…åœ¨ local æ¨¡å¼ä¸‹éœ€è¦ï¼Œä½†ä¹Ÿä¿ç•™å¼•ç”¨é˜²æ­¢æŠ¥é”™)
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
from vllm import LLM, SamplingParams

# LlamaIndex æ ¸å¿ƒç»„ä»¶
from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import NodeWithScore
from llama_index.core.llms import ChatMessage

# OpenAI SDK (ç”¨äº API æ¨¡å¼)
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None # å®¹é”™å¤„ç†

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# ================= é…ç½®ç±» =================

@dataclass
class EnhancedConfig:
    """å¢å¼ºé…ç½®ç±»"""
    
    # --- è¿è¡Œæ¨¡å¼é€‰æ‹© ---
    # å¯é€‰: "local" (ä½¿ç”¨æœ¬åœ°æ˜¾å¡åŠ è½½æ¨¡å‹) æˆ– "api" (è°ƒç”¨è¿œç¨‹æ¥å£)
    MODE = "local" 
    
    # === æœ¬åœ°æ¨¡å‹è·¯å¾„ (MODE="local" æ—¶ç”Ÿæ•ˆ) ===
    EMBEDDING_MODEL = "/root/lanyun-fs/models/Qwen3-Embedding-0.6B"
    RERANKER_MODEL = "/root/lanyun-fs/models/Qwen3-Reranker-0.6B"
    GENERATION_MODEL = "/root/lanyun-tmp/models/Qwen3-4B"
    
    # === API é…ç½® (MODE="api" æ—¶ç”Ÿæ•ˆ) ===
    # 1. LLM API (å…¼å®¹ OpenAI æ ¼å¼, å¦‚ DeepSeek, Moonshot, ChatGPT)
    API_BASE_URL = "https://api.deepseek.com/v1" 
    API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    API_LLM_MODEL_NAME = "deepseek-chat"
    
    # 2. Embedding API
    API_EMBED_BASE_URL = "https://api.openai.com/v1" # æˆ–å…¶ä»–å…¼å®¹åœ°å€
    API_EMBED_KEY = "sk-xxxxxxxx"
    API_EMBED_MODEL_NAME = "text-embedding-3-small"
    
    # 3. Reranker API (é€šç”¨ HTTP æ¥å£ï¼Œå¦‚ SiliconFlow, Jina)
    API_RERANK_URL = "https://api.siliconflow.cn/v1/rerank"
    API_RERANK_KEY = "sk-xxxxxxxx"
    API_RERANK_MODEL_NAME = "BAAI/bge-reranker-v2-m3"

    # --- åŸºç¡€å‚æ•° ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    CHUNK_SIZE = 512  
    CHUNK_OVERLAP = 128
    
    # --- ç¼“å­˜è·¯å¾„ ---
    CACHE_DIR = "../data/FAISS_Vector_DB"
    NODES_CACHE_FILE = "nodes_cache.pkl"
    DOC_STORE_CACHE_FILE = "doc_store_cache.pkl"
    
    # --- æ£€ç´¢å‚æ•° ---
    SIMILARITY_TOP_K = 30  
    BM25_TOP_K = 30
    RERANK_TOP_N = 5       
    
    # --- ç”Ÿæˆå‚æ•° ---
    MAX_NEW_TOKENS = 4096
    TEMPERATURE = 0.1 
    TOP_P = 0.95
    # æ³¨æ„ï¼šæœ¬åœ° vllm å‚æ•°
    gpu_memory_utilization = 0.8
    max_model_len = 32000
    
    # æ§åˆ¶ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦
    MAX_CONTEXT_LENGTH = 25000 
    
    # --- åŠŸèƒ½å¼€å…³ ---
    QUERY_REWRITE_ENABLED = False
    BM25_ENABLED = True
    RERANKER_ENABLED = True
    RRF_ENABLED = True
    RRF_K = 60

    # --- è¾“å‡ºå­—æ®µå®šä¹‰ ---
    OUTPUT_COLUMNS = [
        'å§“åï¼ˆä¸­æ–‡ï¼‰', 'å§“åï¼ˆåŸæ–‡ï¼‰', 'å›½ç±', 'æ°‘æ—/ç§æ—', 'è¯­ç§', 
        'å‡ºç”Ÿå¹´ä»½(å¦‚1950å¹´)', 'å‡ºç”Ÿåœ°', 'èº«ä½“çŠ¶å†µï¼ˆè¿‘å¹´ï¼‰', 
        'ä»»èŒä¿¡æ¯', 'æ•™è‚²ä¿¡æ¯', 'å®¶åº­å…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰', 'ç¤¾ä¼šå…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰', 
        'ç¤¾äº¤ç½‘ç»œ', 'ç¤¾ä¼šå½±å“', 'ä¸ªäººé‡è¦æˆå°±/è‘—ä½œ', 'å…³é”®äº‹ä»¶', 'é‡è¦æ´»åŠ¨', 
        'å…´è¶£åå¥½', 'æ€§æ ¼ç±»å‹å€¾å‘', 'æ€§æ ¼å¼±ç‚¹', 'ä»·å€¼å–å‘', 
        'èŒä¸šè·¯å¾„åˆ†æ', 'æœªæ¥å‘å±•é¢„æœŸ'
    ]
    
    # --- æå–å­—æ®µé…ç½® ---
    EXTRACT_GROUPS = {
        "åŸºç¡€èº«ä»½": [
            'å§“åï¼ˆåŸæ–‡ï¼‰', 'å›½ç±', 'æ°‘æ—/ç§æ—', 'è¯­ç§', 
            'å‡ºç”Ÿå¹´ä»½(å¦‚1950å¹´)', 'å‡ºç”Ÿåœ°', 'èº«ä½“çŠ¶å†µï¼ˆè¿‘å¹´ï¼‰', 
            'å®¶åº­å…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰', 'ç¤¾ä¼šå…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰'
        ],
        "ç”Ÿæ¶¯æˆå°±": [
            'ä»»èŒä¿¡æ¯', 'æ•™è‚²ä¿¡æ¯', 'ç¤¾äº¤ç½‘ç»œ', 'ç¤¾ä¼šå½±å“', 
            'ä¸ªäººé‡è¦æˆå°±/è‘—ä½œ', 'å…³é”®äº‹ä»¶', 'é‡è¦æ´»åŠ¨'
        ],
        "æ·±åº¦ç”»åƒ": [
            'å…´è¶£åå¥½', 'æ€§æ ¼ç±»å‹å€¾å‘', 'æ€§æ ¼å¼±ç‚¹', 'ä»·å€¼å–å‘', 
            'èŒä¸šè·¯å¾„åˆ†æ', 'æœªæ¥å‘å±•é¢„æœŸ'
        ]
    }

    # --- Few-Shot èŒƒä¾‹æ•°æ® ---
    FEW_SHOT_EXAMPLES = {
        "åŸºç¡€èº«ä»½": """
{
  "å§“åï¼ˆåŸæ–‡ï¼‰": "Daniel Newham",
  "å›½ç±": "è‹±å›½",
  "æ°‘æ—/ç§æ—": "è‹±å¾·ä¸¤å›½è¡€ç»Ÿ",
  "è¯­ç§": "è‹±è¯­ã€æ±‰è¯­ã€å¾·è¯­ã€æ³•è¯­",
  "å‡ºç”Ÿå¹´ä»½(å¦‚1950å¹´)": "1980å¹´",
  "å‡ºç”Ÿåœ°": "è‹±å›½åˆ‡å°”æ»•çº³å§†é•‡",
  "èº«ä½“çŠ¶å†µï¼ˆè¿‘å¹´ï¼‰": "è¿‘å¹´å…¬å¼€ä¿¡æ¯æ˜¾ç¤ºä½“æ€ç¨³å¥ï¼Œæ— æ˜¾è‘—å¥åº·é—®é¢˜",
  "å®¶åº­å…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰": "æœªçŸ¥",
  "ç¤¾ä¼šå…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰": "åŒäº‹ï¼šå…‹è±å¥¥Â·å•ç™»"
}
""",
        "ç”Ÿæ¶¯æˆå°±": """
{
  "ä»»èŒä¿¡æ¯": "ç°ä»»èŒåŠ¡ï¼šå¤§ç‰›ï¼ˆDaniel Newhamï¼‰ï¼Œæ³•å­¦ç¡•å£«ï¼ŒèŒä¸šä¸»æŒäººï¼Œæ¸…åå¤§å­¦è‰ºæœ¯å­¦é™¢åšå£«\\næ›¾ä»»ï¼š\\nä¸é˜¿åŠªæ‹‰åŠåŒ—è¯­ä¸€å¥³ç”Ÿä»£è¡¨åŒ—äº¬é˜Ÿå‚åŠ åŒ—äº¬ç”µè§†å°ã€Šç¬¬ä¸‰å±Šä¸­å›½é€šç”µè§†å¤§èµ›ã€‹ï¼ˆ2000å¹´9æœˆï¼‰\\næ±Ÿè‹å«è§†çš„ã€Šé’æ˜¥å¤§ç¢°æ’ã€‹ä½œå˜‰å®¾ä¸»æŒï¼ˆ2001å¹´9æœˆ-12æœˆï¼‰\\nä»£è¡¨æ±Ÿè‹é˜Ÿå‚åŠ ã€Šæ˜¥èŠ‚å¤–å›½äººä¸­åæ‰è‰ºå¤§èµ›ã€‹çš„æˆæ›²å’Œæ›²è‰ºé¡¹ç›®ï¼Œè·å¾—æœ€ä½³è¡¨æ¼”å¥–ï¼ˆ2002å¹´9æœˆï¼‰\\nä¸­å¤®ç”µè§†å°å›½é™…é¢‘é“ã€ŠåŒä¹äº”æ´²ã€‹ä¸»æŒï¼ˆ2002å¹´8æœˆï¼‰\\nä¸­å¤®ç”µè§†å°å›½é™…é¢‘é“ã€Šå­¦æ±‰è¯­â”€å¿«ä¹ä¸­å›½ã€‹ä¸»æŒï¼ˆ2003å¹´3æœˆï¼‰",
  "æ•™è‚²ä¿¡æ¯": "åˆ‡å°”æ»•çº³å§†ä¼¯æ©èµ›å¾·å­¦æ ¡ï¼ˆ1992å¹´9æœˆï½1997å¹´7æœˆï¼‰\\nè‹±å›½åˆ‡å°”æ»•çº³å§†ä½©èŒ¨æ–‡æ³•å­¦æ ¡å…­å¹´çº§ï¼ˆé«˜ä¸­éƒ¨ï¼‰ï¼ˆ1997å¹´9æœˆï½1999å¹´7æœˆï¼‰\\nåœ¨æœä¼¦å¤§å­¦ä¸œäºšç ”ç©¶ç³»å­¦ä¹ æ±‰è¯­ï¼ˆ1999å¹´10æœˆï½2000å¹´7æœˆï¼‰\\nä¸­å›½äººæ°‘å¤§å­¦è¿›è¡Œä¸€å¹´çš„æ±‰è¯­åŸ¹è®­ï¼ˆ2000å¹´9æœˆï½2001å¹´7æœˆï¼‰\\nä¸­å›½äººæ°‘å¤§å­¦ æ–‡å­¦å­¦å£«ï¼ˆ2000å¹´ - 2004å¹´ï¼‰\\nä¸­å›½äººæ°‘å¤§å­¦ æ³•å­¦é™¢ â€” æ³•å­¦ç¡•å£« LLMï¼ˆ2014å¹´ - 2016å¹´ï¼‰\\næ¸…åå¤§å­¦åšå£«ç ”ç©¶ç”Ÿ(2020å¹´9æœˆ - è‡³ä»Š)",
  "ç¤¾äº¤ç½‘ç»œ": "ä¸ä¸­å›½äººæ°‘å¤§å­¦ä¿æŒç´§å¯†è”ç³»ï¼Œä½œä¸ºæ ¡å‹ä»£è¡¨å‡ºå¸­å›½é™…æ–‡åŒ–äº¤æµå­¦é™¢æˆç«‹å¤§ä¼šå¹¶å‘è¡¨è®²è¯ã€‚",
  "ç¤¾ä¼šå½±å“": "è¢«ä¸­å›½åª’ä½“ç§°ä¸ºâ€˜ä¸­å›½é€šâ€™ï¼Œå‚ä¸åª’ä½“ä¸æ´»åŠ¨ä¼ æ’­ä¸­åæ–‡åŒ–ï¼›è¢«èª‰ä¸ºâ€œæœ€ä¼šè¯´æ±‰è¯­çš„å¤–å›½ä¸»æŒäººâ€ã€‚",
  "ä¸ªäººé‡è¦æˆå°±/è‘—ä½œ": "ä¸»æŒä¸­å¤®ç”µè§†å°å›½é™…é¢‘é“èŠ‚ç›®ã€ŠåŒä¹äº”æ´²ã€‹ã€Šå¿«ä¹ä¸­å›½â€”â€”å­¦æ±‰è¯­ã€‹ï¼›è·å¾—ä¸­å›½äººæ°‘å¤§å­¦æ–‡å­¦å­¦å£«ä¸æ³•å­¦ç¡•å£«å­¦ä½ï¼›2025å¹´å‚ä¸çºªå½•ç‰‡ã€Šæ–‡è¿ä¸­å›½ã€‹æ‹æ‘„ï¼Œæ¨åŠ¨ä¸­å›½æ–‡åŒ–å›½é™…ä¼ æ’­ã€‚",
  "å…³é”®äº‹ä»¶": "2000â€“2001å¹´ï¼šèµ´ä¸­å›½äººæ°‘å¤§å­¦å­¦ä¹ ä¸­æ–‡è¯­è¨€æ–‡å­¦ï¼Œå¼€å¯åœ¨åç”Ÿæ´»ã€‚\\n2004å¹´èµ·ï¼šåŠ å…¥ä¸­å¤®ç”µè§†å°å›½é™…é¢‘é“ï¼Œä¸»æŒã€ŠåŒä¹äº”æ´²ã€‹ã€‚\\n2014å¹´ï¼šè¿›å…¥ä¸­å›½äººæ°‘å¤§å­¦æ³•å­¦é™¢æ”»è¯»æ³•å­¦ç¡•å£«ã€‚\\n2025å¹´ï¼šä¸æ³•å›½ä¸»æŒäººå…‹è±å¥¥Â·å•ç™»å…±åŒå‡ºæ¼”çºªå½•ç‰‡ã€Šæ–‡è¿ä¸­å›½ã€‹ã€‚",
  "é‡è¦æ´»åŠ¨": "2014 å¹´å€¡å¯¼æ±‰è¯­å­¦ä¹ ä¸æ–‡åŒ–ç†è§£ï¼›2025 å¹´é€šè¿‡çºªå½•ç‰‡ã€Šæ–‡è¿ä¸­å›½ã€‹å‘¼åå…¨çƒè§‚ä¼—äº²èº«æ„Ÿå—ä¸­å›½ã€‚"
}
""",
        "æ·±åº¦ç”»åƒ": """
{
  "å…´è¶£åå¥½": "è¡¨æ¼”è¯å‰§ï¼Œæ¼”ç”µå½±å¼¹é’¢ç´ï¼Œå¹å°å·ï¼Œå¼¹å¤ç´ï¼Œå–œçˆ±æˆæ›²",
  "æ€§æ ¼ç±»å‹å€¾å‘": "å¤–å‘å¼€æ”¾å‹",
  "æ€§æ ¼å¼±ç‚¹": "å…·æœ‰æ˜æ˜¾çš„ç†æƒ³ä¸»ä¹‰å€¾å‘ã€èˆ†è®ºå‹åŠ›å¤§ã€è‡ªæˆ‘è¦æ±‚é«˜",
  "ä»·å€¼å–å‘": "æ–‡åŒ–è®¤åŒï¼šè®¤ä¸ºè‡ªå·±æ˜¯ä¸€ä¸ªâ€œä¸­å›½ä¸»ä¹‰è€…â€ï¼Œæ¨å´‡ä¸­å›½æ–‡åŒ–ä¸ä¼ ç»Ÿè‰ºæœ¯ï¼ˆä¹¦æ³•ã€å›½å­¦ã€æˆæ›²ç­‰ï¼‰",
  "èŒä¸šè·¯å¾„åˆ†æ": "æ–‡åŒ–ä¼ æ’­å‹ï¼šè¯­è¨€æ–‡åŒ–æ·±è€•ï¼ˆ1999å¹´èµ·ï¼‰â†’ ä¼ åª’ä¼ æ’­å®è·µï¼ˆ2003å¹´èµ·ï¼Œè¿›å…¥å¤®è§†ï¼‰â†’ æ–‡åŒ–äº¤æµé˜¶æ®µï¼ˆè‡³ä»Šï¼Œæ´»è·ƒäºé«˜æ ¡ä¸å›½é™…äº¤æµæ´»åŠ¨ï¼‰ã€‚",
  "æœªæ¥å‘å±•é¢„æœŸ": "å‹åäººç‰©ï¼šé¢„è®¡æœªæ¥ä¹Ÿä¼šä¸ä¸­å›½å‹å¥½ï¼›æ–‡åŒ–ä¼ æ’­è€…ï¼šå¸Œæœ›æœªæ¥èƒ½â€œé‚€è¯·æ›´å¤šå…¨çƒè§‚ä¼—æ¥ä¸­å›½â€ï¼Œç»§ç»­å¼ºè°ƒè·¨æ–‡åŒ–ç†è§£ã€‚"
}
"""
    }

# ================= è¾…åŠ©ç»„ä»¶ç±» =================
class KeywordExtractor:
    @staticmethod
    def extract(text: str, top_k: int = 10) -> List[str]:
        if not text: return []
        # TextRank é€‚åˆæå–åè¯çŸ­è¯­ï¼Œæ›´é€‚åˆä½œä¸ºå…³é”®è¯
        keywords = jieba.analyse.textrank(text, topK=top_k, withWeight=False, allowPOS=('ns', 'n', 'vn', 'nr', 'nt'))
        if len(keywords) < 2:
            keywords = jieba.analyse.extract_tags(text, topK=top_k)
        return keywords

class ReciprocalRankFusion:
    def __init__(self, k=60): self.k = k
    
    def fuse(self, ranked_lists, weights=None):
        """æ”¯æŒåŠ æƒçš„ RRF èåˆ"""
        if not ranked_lists: return []
        
        if weights is None:
            weights = [1.0] * len(ranked_lists)
            
        if len(weights) != len(ranked_lists):
            weights = [1.0] * len(ranked_lists)

        scores = {}
        all_nodes = {}
        
        for lst, w in zip(ranked_lists, weights):
            for rank, node in enumerate(lst):
                nid = node.node.node_id
                all_nodes[nid] = node.node
                # åŠ æƒ RRF å…¬å¼
                scores[nid] = scores.get(nid, 0) + w * (1.0 / (self.k + rank + 1))
        
        fused = [NodeWithScore(node=all_nodes[nid], score=score) for nid, score in scores.items()]
        fused.sort(key=lambda x: x.score, reverse=True)
        return fused

# ================= æ¨¡å‹å°è£…ç±» (Local) =================
class Qwen3Embedding:
    """æœ¬åœ° Embedding æ¨¡å‹"""
    def __init__(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=EnhancedConfig.DEVICE)
        self.model.eval()

    def get_text_embeddings(self, texts):
        with torch.no_grad():
            inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512).to(EnhancedConfig.DEVICE)
            outputs = self.model(**inputs)
            embeddings = self._mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])
        return embeddings.cpu().numpy().tolist()
    
    def get_query_embedding(self, query): return self.get_text_embeddings([query])[0]
    
    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

class Qwen3DirectReranker:
    """æœ¬åœ° Reranker æ¨¡å‹"""
    def __init__(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=EnhancedConfig.DEVICE)
        self.model.eval()

    def rerank(self, query, nodes, top_n):
        if not nodes: return []
        scored_nodes = []
        for node in nodes:
            text_snippet = node.node.text[:512]
            prompt = f"æŸ¥è¯¢: {query}\næ–‡æ¡£: {text_snippet}"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(EnhancedConfig.DEVICE)
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits[:, -1, :]
                yes_id = self.tokenizer.encode("yes", add_special_tokens=False)[0]
                no_id = self.tokenizer.encode("no", add_special_tokens=False)[0]
                score = np.exp(logits[0, yes_id].item()) / (np.exp(logits[0, yes_id].item()) + np.exp(logits[0, no_id].item()) + 1e-9)
            node.score = score
            scored_nodes.append(node)
        return sorted(scored_nodes, key=lambda x: x.score, reverse=True)[:top_n]

class SimpleLLM:
    """æœ¬åœ° vLLM æ¨¡å‹"""
    def __init__(self, model_path):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.llm = LLM(model=model_path, 
                       gpu_memory_utilization=EnhancedConfig.gpu_memory_utilization, 
                       max_model_len=EnhancedConfig.max_model_len, 
                       trust_remote_code=True,
                       tensor_parallel_size=1)
        self.sampling_params = SamplingParams(
            temperature=EnhancedConfig.TEMPERATURE, 
            max_tokens=EnhancedConfig.MAX_NEW_TOKENS, 
            top_p=EnhancedConfig.TOP_P
        )

    def chat(self, messages: List[ChatMessage]) -> ChatMessage:
        prompt = self.tokenizer.apply_chat_template(
            [{"role": m.role, "content": m.content} for m in messages], 
            tokenize=False, 
            add_generation_prompt=True
        )
        outputs = self.llm.generate([prompt], self.sampling_params)
        return ChatMessage(role="assistant", content=outputs[0].outputs[0].text)

# ================= æ¨¡å‹å°è£…ç±» (API) =================

class OpenAIEmbedding:
    """API æ¨¡å¼: Embedding"""
    def __init__(self, api_key, base_url, model_name):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name

    def get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        # OpenAI API æœ‰ batch size é™åˆ¶ï¼Œé€šå¸¸ä¸€æ¬¡ä¸è¶…è¿‡ 2048 ä¸ª token æˆ–ä¸€å®šæ•°é‡çš„ string
        # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœ texts å¤ªå¤šå¯ä»¥è‡ªè¡Œåˆ†æ‰¹
        texts = [t.replace("\n", " ") for t in texts] # æ¨èæ“ä½œï¼šç§»é™¤æ¢è¡Œ
        response = self.client.embeddings.create(input=texts, model=self.model_name)
        return [data.embedding for data in response.data]

    def get_query_embedding(self, query: str) -> List[float]:
        return self.get_text_embeddings([query])[0]

class OpenAILLM:
    """API æ¨¡å¼: LLM"""
    def __init__(self, api_key, base_url, model_name):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name

    def chat(self, messages: List[ChatMessage]) -> ChatMessage:
        # å°† LlamaIndex çš„ ChatMessage è½¬æ¢ä¸º OpenAI çš„ dict æ ¼å¼
        openai_messages = [{"role": m.role, "content": m.content} for m in messages]
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=openai_messages,
            temperature=EnhancedConfig.TEMPERATURE,
            max_tokens=EnhancedConfig.MAX_NEW_TOKENS,
            top_p=EnhancedConfig.TOP_P
        )
        content = response.choices[0].message.content
        return ChatMessage(role="assistant", content=content)

class APIReranker:
    """API æ¨¡å¼: Reranker (é€šç”¨ HTTP æ¥å£)"""
    def __init__(self, api_key, api_url, model_name):
        self.api_key = api_key
        self.api_url = api_url
        self.model_name = model_name

    def rerank(self, query, nodes, top_n):
        if not nodes: return []
        
        # æå–æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
        documents = [n.node.text for n in nodes]
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model_name,
            "query": query,
            "documents": documents,
            "top_n": top_n
        }
        
        try:
            response = requests.post(self.api_url, json=payload, headers=headers, timeout=10)
            response.raise_for_status()
            results = response.json().get("results", [])
            
            # å°† API è¿”å›ç»“æœæ˜ å°„å› NodeWithScore
            scored_nodes = []
            for res in results:
                idx = res["index"]
                score = res["relevance_score"]
                node = nodes[idx]
                node.score = score
                scored_nodes.append(node)
            
            return sorted(scored_nodes, key=lambda x: x.score, reverse=True)
            
        except Exception as e:
            print(f"âš ï¸ Rerank API è°ƒç”¨å¤±è´¥: {e}ï¼Œå°†è¿”å›åŸå§‹é¡ºåºçš„å‰ {top_n} ä¸ªã€‚")
            return nodes[:top_n]

# ================= æ•°æ®å¤„ç† =================
class GlobalDocumentStore:
    """
    å…¨å±€æ–‡æ¡£å­˜å‚¨
    _store ç»“æ„ä¼˜åŒ–ä¸º: { doc_id: {'text': str, 'metadata': dict} }
    ä»¥æ”¯æŒè¿”å›å®Œæ•´ Node å¯¹è±¡ç»™ _build_context_str
    """
    _store = {} 
    
    @classmethod
    def add_document(cls, doc_id, text, metadata=None):
        if metadata is None: metadata = {}
        cls._store[doc_id] = {'text': text, 'metadata': metadata}
        
    @classmethod
    def get_document_data(cls, doc_id):
        """è¿”å›åŒ…å«æ–‡æœ¬å’Œå…ƒæ•°æ®çš„å®Œæ•´æ•°æ®åŒ…"""
        return cls._store.get(doc_id, {'text': "", 'metadata': {}})
    
    @classmethod
    def get_document(cls, doc_id):
        """å…¼å®¹æ—§æ¥å£ï¼Œåªè¿”å›æ–‡æœ¬"""
        return cls._store.get(doc_id, {}).get('text', "")

class EnhancedDocumentProcessor:
    def read_all_documents(self, base_path):
        all_docs = []
        data_path = os.path.join(base_path, "æ¸…æ´—æ•°æ®")
        if not os.path.exists(data_path):
            data_path = os.path.join(base_path, "data")
            
        print(f"æ­£åœ¨ä» {data_path} è¯»å–æ–‡æ¡£...")
        for root, _, files in os.walk(data_path):
            for file in files:
                fpath = os.path.join(root, file)
                try:
                    content = ""
                    if file.endswith('.txt') or file.endswith('.md') or file.endswith('.csv'):
                        with open(fpath, 'r', errors='ignore', encoding='utf-8') as f: content = f.read()
                    elif file.endswith('.docx'):
                        doc = docx.Document(fpath)
                        content = "\n".join([p.text for p in doc.paragraphs])
                    
                    if content.strip():
                        doc_id = str(uuid.uuid4())
                        # ç¡®ä¿å…ƒæ•°æ®ä¸­åŒ…å« file_name
                        metadata = {"doc_id": doc_id, "file_name": file}
                        doc_obj = Document(text=content, metadata=metadata)
                        all_docs.append(doc_obj)
                        
                        # å­˜å…¥å…¨å±€ Store
                        GlobalDocumentStore.add_document(doc_id, content, metadata=metadata)
                        
                except Exception as e:
                    print(f"Skipped {file}: {e}")
        return all_docs

class SimpleVectorStore:
    def __init__(self, nodes, embed_model, index_path):
        self.nodes = nodes
        self.embed_model = embed_model
        self.index_path = index_path
        if os.path.exists(index_path):
            print(f"åŠ è½½ç°æœ‰å‘é‡ç´¢å¼•: {index_path}")
            self.index = faiss.read_index(index_path)
        else:
            self._build_index()

    def _build_index(self):
        print("æ„å»ºå‘é‡ç´¢å¼•...")
        if not self.nodes: return
        batch_size = 32
        texts = [n.text for n in self.nodes]
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            embeddings.extend(self.embed_model.get_text_embeddings(batch))
        arr = np.array(embeddings, dtype=np.float32)
        faiss.normalize_L2(arr)
        self.index = faiss.IndexFlatIP(arr.shape[1])
        self.index.add(arr)
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        faiss.write_index(self.index, self.index_path)

    def search(self, query, top_k):
        if not self.nodes: return []
        q_emb = np.array([self.embed_model.get_query_embedding(query)], dtype=np.float32)
        faiss.normalize_L2(q_emb)
        sims, idxs = self.index.search(q_emb, top_k)
        return [NodeWithScore(node=self.nodes[i], score=float(s)) for s, i in zip(sims[0], idxs[0]) if i != -1]

# ================= æ ¸å¿ƒç³»ç»Ÿç±» =================
class EnhancedHybridRetriever:
    def __init__(self, vector_store, nodes, reranker):
        self.vector_store = vector_store
        self.nodes = nodes
        self.reranker = reranker
        self.bm25 = None
        if EnhancedConfig.BM25_ENABLED and nodes:
            print("æ„å»º BM25 ç´¢å¼•...")
            tokenized_corpus = [jieba.lcut(n.text) for n in nodes]
            self.bm25 = BM25Okapi(tokenized_corpus)
        self.rrf = ReciprocalRankFusion(k=EnhancedConfig.RRF_K)

    def retrieve(self, query_name, original_query):
        # 1. ç»Ÿä¸€è½¬ä¸ºåˆ—è¡¨å¤„ç†
        queries = original_query if isinstance(original_query, list) else [original_query]
        rerank_query_text = queries[0] 

        # å®šä¹‰èåˆæƒé‡
        VECTOR_WEIGHT = 1.0
        BM25_WEIGHT = 0.5

        # é˜¶æ®µ 1: å¤šè·¯æ··åˆæ£€ç´¢
        all_result_lists = []
        all_weights = []

        for q in queries:
            # A. å‘é‡æ£€ç´¢
            vec_nodes = self.vector_store.search(q, EnhancedConfig.SIMILARITY_TOP_K)
            all_result_lists.append(vec_nodes)
            all_weights.append(VECTOR_WEIGHT)
            
            # B. BM25 æ£€ç´¢
            if self.bm25:
                if any('\u4e00' <= char <= '\u9fff' for char in q):
                    tokenized_query = jieba.lcut(q)
                else:
                    tokenized_query = q.split() 
                
                scores = self.bm25.get_scores(tokenized_query)
                top_n_indices = np.argsort(scores)[::-1][:EnhancedConfig.BM25_TOP_K]
                
                bm25_nodes = [NodeWithScore(node=self.nodes[i], score=float(scores[i])) 
                              for i in top_n_indices if scores[i] > 1.0]
                
                all_result_lists.append(bm25_nodes)
                all_weights.append(BM25_WEIGHT)
        
        # C. åŠ æƒ RRF èåˆ
        chunk_candidates_list = self.rrf.fuse(all_result_lists, weights=all_weights)
        
        # é˜¶æ®µ 2: å…³é”®è¯å…ƒæ•°æ®å¢å¼º
        final_scored_chunks = []
        for item in chunk_candidates_list:
            node = item.node
            score = item.score
            node_keywords = node.metadata.get('keywords', [])
            
            hit_keyword = False
            for kw in node_keywords:
                if query_name in kw or kw in query_name:
                    hit_keyword = True
                    break
            
            if hit_keyword:
                score *= 2.0 
            
            item.score = score
            final_scored_chunks.append(item)
            
        final_scored_chunks.sort(key=lambda x: x.score, reverse=True)

        # é˜¶æ®µ 3: é‡æ’åº
        if self.reranker:
            final_scored_chunks = self.reranker.rerank(rerank_query_text, final_scored_chunks, top_n=20)

        # é˜¶æ®µ 4: çˆ¶æ–‡æ¡£æ˜ å°„ (Small-to-Big)
        doc_scores = {}
        for item in final_scored_chunks:
            doc_id = item.node.metadata.get('doc_id')
            if not doc_id: continue
            if item.score > doc_scores.get(doc_id, 0):
                doc_scores[doc_id] = item.score

        sorted_doc_ids = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:EnhancedConfig.RERANK_TOP_N]
        
        # ã€å…³é”®ä¿®æ”¹ã€‘è¿”å› NodeWithScore å¯¹è±¡åˆ—è¡¨ï¼Œä»¥ä¾¿ _build_context_str å¤„ç†
        final_parent_nodes = []
        for doc_id, score in sorted_doc_ids:
            doc_data = GlobalDocumentStore.get_document_data(doc_id)
            if not doc_data.get('text'): continue
            
            # é‡å»º Node (å¸¦å…ƒæ•°æ®)
            full_node = Document(text=doc_data['text'], metadata=doc_data.get('metadata', {}))
            final_parent_nodes.append(NodeWithScore(node=full_node, score=score))
            
        return final_parent_nodes

class FullyOptimizedQASystem:
    def __init__(self):
        self.vector_store = None
        self.retriever = None
        self.llm = None
        self.embed_model = None
        self.reranker = None
        self.nodes = []
        self.faiss_path = '../data/FAISS_Vector_DB/faiss_index.bin'

    def initialize(self, doc_path: str):
        print(f"1. åˆå§‹åŒ–ç³»ç»Ÿ (æ¨¡å¼: {EnhancedConfig.MODE})...")
        
        # === 1. åŠ è½½æ¨¡å‹ (Local vs API) ===
        if EnhancedConfig.MODE == "local":
            print("   æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹ (Qwen3)...")
            self.embed_model = Qwen3Embedding(EnhancedConfig.EMBEDDING_MODEL)
            self.llm = SimpleLLM(EnhancedConfig.GENERATION_MODEL)
            if EnhancedConfig.RERANKER_ENABLED:
                self.reranker = Qwen3DirectReranker(EnhancedConfig.RERANKER_MODEL)
        else:
            print("   æ­£åœ¨åˆå§‹åŒ– API å®¢æˆ·ç«¯...")
            if OpenAI is None: raise ImportError("ä½¿ç”¨ API æ¨¡å¼è¯·å…ˆå®‰è£… `pip install openai`")
            
            self.embed_model = OpenAIEmbedding(
                api_key=EnhancedConfig.API_EMBED_KEY,
                base_url=EnhancedConfig.API_EMBED_BASE_URL,
                model_name=EnhancedConfig.API_EMBED_MODEL_NAME
            )
            self.llm = OpenAILLM(
                api_key=EnhancedConfig.API_KEY,
                base_url=EnhancedConfig.API_BASE_URL,
                model_name=EnhancedConfig.API_LLM_MODEL_NAME
            )
            if EnhancedConfig.RERANKER_ENABLED:
                self.reranker = APIReranker(
                    api_key=EnhancedConfig.API_RERANK_KEY,
                    api_url=EnhancedConfig.API_RERANK_URL,
                    model_name=EnhancedConfig.API_RERANK_MODEL_NAME
                )

        # === 2. ç¼“å­˜å¤„ç†ä¸æ–‡æ¡£åŠ è½½ ===
        # --- ç¼“å­˜é€»è¾‘å¼€å§‹ ---
        os.makedirs(EnhancedConfig.CACHE_DIR, exist_ok=True)
        nodes_cache_path = os.path.join(EnhancedConfig.CACHE_DIR, EnhancedConfig.NODES_CACHE_FILE)
        doc_store_cache_path = os.path.join(EnhancedConfig.CACHE_DIR, EnhancedConfig.DOC_STORE_CACHE_FILE)
        
        # å°è¯•åŠ è½½ç¼“å­˜
        if os.path.exists(nodes_cache_path) and os.path.exists(doc_store_cache_path):
            print(f"2. æ£€æµ‹åˆ°èŠ‚ç‚¹ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½: {nodes_cache_path} ...")
            try:
                with open(nodes_cache_path, 'rb') as f:
                    self.nodes = pickle.load(f)
                with open(doc_store_cache_path, 'rb') as f:
                    GlobalDocumentStore._store = pickle.load(f)
                print(f"âœ“ æˆåŠŸåŠ è½½ {len(self.nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ ({e})ï¼Œå°†é‡æ–°å¤„ç†æ–‡æ¡£...")
                self._process_and_cache_docs(doc_path, nodes_cache_path, doc_store_cache_path)
        else:
            print("2. æœªæ£€æµ‹åˆ°ç¼“å­˜ï¼Œå¼€å§‹å…¨é‡å¤„ç†æ–‡æ¡£...")
            self._process_and_cache_docs(doc_path, nodes_cache_path, doc_store_cache_path)
        # --- ç¼“å­˜é€»è¾‘ç»“æŸ ---

        print("4. æ„å»º/åŠ è½½å‘é‡ç´¢å¼• (FAISS)...")
        self.vector_store = SimpleVectorStore(self.nodes, self.embed_model, self.faiss_path)
        
        self.retriever = EnhancedHybridRetriever(self.vector_store, self.nodes, self.reranker)
        print("âœ“ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _process_and_cache_docs(self, doc_path, nodes_path, doc_store_path):
        """å¤„ç†æ–‡æ¡£ã€æå–å…³é”®è¯å¹¶ä¿å­˜ç¼“å­˜çš„è¾…åŠ©å‡½æ•°"""
        print("   (a) è¯»å–æºæ–‡ä»¶...")
        processor = EnhancedDocumentProcessor()
        docs = processor.read_all_documents(doc_path)
        
        print("   (b) æ–‡æ¡£åˆ‡åˆ†ä¸å…³é”®è¯æå– (æ­¤æ­¥éª¤è¾ƒæ…¢)...")
        splitter = SentenceSplitter(chunk_size=EnhancedConfig.CHUNK_SIZE, chunk_overlap=EnhancedConfig.CHUNK_OVERLAP)
        self.nodes = []
        
        for d in docs:
            doc_id = d.metadata['doc_id']
            cur_nodes = splitter.get_nodes_from_documents([d])
            for n in cur_nodes:
                n.metadata['doc_id'] = doc_id
                # ç¡®ä¿ä¼ é€’ file_nameï¼Œä¾› _build_context_str ä½¿ç”¨
                n.metadata['file_name'] = d.metadata.get('file_name', 'unknown')
                n.metadata['keywords'] = KeywordExtractor.extract(n.text, top_k=5)
            self.nodes.extend(cur_nodes)
            
        print(f"   (c) ç”Ÿæˆ {len(self.nodes)} ä¸ªç´¢å¼•èŠ‚ç‚¹ã€‚")
        
        print("   (d) æ­£åœ¨ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜...")
        with open(nodes_path, 'wb') as f:
            pickle.dump(self.nodes, f)
        with open(doc_store_path, 'wb') as f:
            pickle.dump(GlobalDocumentStore._store, f)
        print("âœ“ ç¼“å­˜ä¿å­˜å®Œæˆã€‚")

    # --- ä»¥ä¸‹ä¸ºæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ ---

    def _post_process(self, answer: str) -> str:
        pattern = r'<think>(.*?)</think>'
        content = re.sub(pattern, '', answer, flags=re.DOTALL)
        return re.sub(r'\n+', '\n', content).strip()

    def _safe_llm(self, prompt, label):
        attempt = 0
        while True:
            attempt += 1
            try:
                raw_res = self.llm.chat([ChatMessage(role="user", content=prompt)]).content
                processed_res = self._post_process(raw_res)
                json_str = self._clean_json(processed_res)
                return json.loads(json_str)
            except Exception as e:
                print(f"âŒ {label} è§£æå¤±è´¥ (ç¬¬ {attempt} æ¬¡å°è¯•): {e}")
                if attempt >= 5: 
                     print("ğŸš« è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡ã€‚")
                     return {}
                continue

    def _clean_json(self, text):
        text = re.sub(r'```json\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'```', '', text)
        s = text.find('{')
        e = text.rfind('}')
        if s != -1 and e != -1:
            return text[s:e+1]
        return "{}"

    def extract_info(self, name: str, context: str) -> Dict:
        all_res = {}
        field_definitions = {
            'å§“åï¼ˆåŸæ–‡ï¼‰': f"äººç‰©ã€{name}ã€‘çš„å¤–æ–‡å…¨åã€åŸåæˆ–æ›¾ç”¨åã€‚éä¸­æ–‡åå¿…é¡»æå–ã€‚",
            'å‡ºç”Ÿå¹´ä»½(å¦‚1950å¹´)': "ä»…æå–æ•°å­—å¹´ä»½ï¼ˆä¾‹å¦‚ï¼š1965ï¼‰ã€‚",
            'å®¶åº­å…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰': "é…å¶ã€å­å¥³ã€çˆ¶æ¯ç­‰ã€‚æ ¼å¼ï¼š'å…³ç³»ï¼šå§“å'ã€‚",
            'ç¤¾ä¼šå…³ç³»ï¼ˆå¦‚â€œå…³ç³»ï¼šäººåâ€ï¼‰': "æ©å¸ˆã€å¯†å‹ã€åˆä½œä¼™ä¼´ã€‚æ ¼å¼ï¼š'å…³ç³»ï¼šå§“å'ã€‚",
            'ä»»èŒä¿¡æ¯': "æ›¾ä»»åŠç°ä»»èŒåŠ¡ã€‚æ ¼å¼ï¼š'æ—¶é—´ï¼šæœºæ„/èŒä½'ã€‚å¤šé¡¹ç”¨åˆ†å·åˆ†éš”ã€‚",
            'æ•™è‚²ä¿¡æ¯': "å­¦ä½ã€æ¯•ä¸šé™¢æ ¡åŠä¸“ä¸šã€‚åŒ…å«ç•™å­¦ç»å†ã€‚",
            'ç¤¾äº¤ç½‘ç»œ': "æ‰€å±åœˆå±‚ã€ä¿±ä¹éƒ¨æˆ–æ ¸å¿ƒäººè„‰ç½‘ç»œã€‚",
            'ç¤¾ä¼šå½±å“': "å½±å“åŠ›è¯„ä¼°ã€è£èª‰å¤´è¡”ã€‚",
            'ä¸ªäººé‡è¦æˆå°±/è‘—ä½œ': "ä»£è¡¨æ€§ä½œå“ã€æˆæœæˆ–å¥–é¡¹ã€‚",
            'å…³é”®äº‹ä»¶': "è½¬æŠ˜æ€§äº‹ä»¶ã€‚",
            'å…´è¶£åå¥½': "ä¸šä½™çˆ±å¥½ã€ç”Ÿæ´»ä¹ æƒ¯ã€‚",
            'æ€§æ ¼ç±»å‹å€¾å‘': "æ ¹æ®è¨€è¡Œæ¨æ–­çš„æ€§æ ¼ç‰¹å¾ã€‚",
            'æ€§æ ¼å¼±ç‚¹': "æ ¹æ®äº‰è®®äº‹ä»¶æ¨æ–­çš„æ€§æ ¼çŸ­æ¿ã€‚",
            'ä»·å€¼å–å‘': "å…¬å¼€è¡¨è¾¾æˆ–è¡ŒåŠ¨ä½“ç°çš„æ ¸å¿ƒä»·å€¼è§‚ã€‚",
            'èŒä¸šè·¯å¾„åˆ†æ': "èŒä¸šä¸Šå‡é€»è¾‘æ€»ç»“ã€‚",
            'æœªæ¥å‘å±•é¢„æœŸ': "åŸºäºç°çŠ¶å¯¹æœªæ¥çš„é¢„æµ‹ã€‚"
        }

        for group_name, fields in EnhancedConfig.EXTRACT_GROUPS.items():
            current_field_descs = {k: field_definitions.get(k, "æ ¹æ®ä¸Šä¸‹æ–‡æå–") for k in fields}
            example_json = EnhancedConfig.FEW_SHOT_EXAMPLES.get(group_name, "{}")
            
            group_instruction = ""
            if group_name == "åŸºç¡€èº«ä»½":
                group_instruction = "ã€æŒ‡ä»¤ã€‘ï¼šæ³¨é‡å‡†ç¡®æ€§ï¼Œä¸¥æ ¼åŒºåˆ†åŒåäººç‰©ã€‚"
            elif group_name == "ç”Ÿæ¶¯æˆå°±":
                group_instruction = "ã€æŒ‡ä»¤ã€‘ï¼šæ³¨é‡æ—¶é—´çº¿ï¼ˆå€’åºæˆ–é¡ºåºï¼‰ï¼Œå¤šé¡¹å†…å®¹ç”¨åˆ†å·åˆ†éš”ã€‚"
            elif group_name == "æ·±åº¦ç”»åƒ":
                group_instruction = "ã€æŒ‡ä»¤ã€‘ï¼šéœ€è¦åŸºäºè¡Œä¸ºè¿›è¡Œæ¨ç†ï¼ˆInferenceï¼‰ï¼Œä¸è¦ä»…æ‘˜æŠ„ã€‚"

            prompt = f"""ä½ æ˜¯ä¸€ä½é«˜çº§æƒ…æŠ¥åˆ†æå¸ˆã€‚åŸºäºèµ„æ–™æ„å»ºäººç‰©ã€{name}ã€‘çš„ã€{group_name}ã€‘æ¡£æ¡ˆã€‚

ã€èƒŒæ™¯èµ„æ–™ã€‘ï¼š
{context[:EnhancedConfig.MAX_CONTEXT_LENGTH]}

ã€å¾…æå–å­—æ®µåŠå®šä¹‰ã€‘ï¼š
{json.dumps(current_field_descs, ensure_ascii=False, indent=2)}

ã€å‚è€ƒèŒƒä¾‹ (Style Guide)ã€‘ï¼š
è¯·ä¸¥æ ¼æ¨¡ä»¿ä»¥ä¸‹ JSON çš„å­—æ®µå¡«å†™é£æ ¼ï¼ˆå°¤å…¶æ˜¯æ—¶é—´çº¿æ ¼å¼å’Œåˆ†å·åˆ†éš”ï¼‰ï¼š
{example_json}

{group_instruction}

ã€è¦æ±‚ã€‘ï¼š
1. ä»…è¾“å‡º JSONã€‚
2. ç¼ºå¤±å­—æ®µå¡«â€œæœªçŸ¥â€ã€‚
3. å¤šæ¡ä¿¡æ¯ç”¨åˆ†å·åˆ†éš”ã€‚

è¯·ç”Ÿæˆ JSONï¼š
"""
            res = self._safe_llm(prompt, group_name)
            for k in fields:
                if k not in res: res[k] = "æœªçŸ¥"
            all_res.update(res)
            
        all_res['å§“åï¼ˆä¸­æ–‡ï¼‰'] = name
        return all_res

    def _translate_name(self, name: str) -> str:
        """
        ä¼˜åŒ–ï¼šæ™ºèƒ½åˆ¤æ–­è¯­ç§çš„ç¿»è¯‘ Prompt
        æ ¹æ®äººç‰©èƒŒæ™¯å†³å®šæ˜¯è¾“å‡ºä¿„è¯­è¿˜æ˜¯è‹±è¯­
        """
        prompt = f"""Task: Identify the real-world person associated with the Chinese name '{name}'.

Instructions:
1. If the person is from a **Russian-speaking country** (e.g., Russia, Ukraine, Belarus, USSR), output their name in **Russian (Cyrillic)**.
2. For all other persons (Western, International, etc.), output their name in **English**.
3. Output **ONLY** the name. Do not include any explanation, punctuation, or extra words.

Target Name:"""
        try:
            raw_res = self.llm.chat([ChatMessage(role="user", content=prompt)]).content
            process_res = self._post_process(raw_res)
            # ç®€å•çš„æ¸…æ´—ï¼Œç§»é™¤å¥å·
            return process_res.strip().replace(".", "").replace("ã€‚", "")
        except:
            return name
    
    # ã€æ–°å¢ã€‘æ„å»ºä¸Šä¸‹æ–‡çš„æ¨¡å—ï¼Œæ§åˆ¶é•¿åº¦å¹¶ä¿ç•™æ¥æº
    def _build_context_str(self, nodes):
        context = []
        added_ids = set()
        cur_len = 0
        
        # ä¼ å…¥çš„ nodes å·²ç»æ˜¯ NodeWithScore åˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°æ’åº
        sorted_nodes = sorted(nodes, key=lambda x: x.score, reverse=True)
        
        for node_score in sorted_nodes:
            node = node_score.node
            # ä½¿ç”¨ file_name å’Œ doc_id è”åˆå»é‡
            key = f"{node.metadata.get('file_name', 'unknown')}_{node.node_id}"
            if key in added_ids: continue
            
            # æ ¼å¼åŒ–æ–‡æœ¬
            text = f"ã€æ¥æº:{node.metadata.get('file_name', 'æœªçŸ¥')}ã€‘\n{node.text}\n"
            
            # ä¸¥æ ¼æ§åˆ¶é•¿åº¦
            if cur_len + len(text) > EnhancedConfig.MAX_CONTEXT_LENGTH: break
            
            context.append(text)
            added_ids.add(key)
            cur_len += len(text)
            
        return "\n".join(context)

    def generate_person_profile(self, name: str) -> Dict:
        print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆäººç‰©ç”»åƒ: {name}")
        
        # 1. ç¿»è¯‘åå­— (ä¿„è¯­/è‹±è¯­ æ™ºèƒ½åˆ¤æ–­)
        translated_name = self._translate_name(name)
        print(f"ğŸ”¤ ç¿»è¯‘ç»“æœ: {translated_name}")

        # 2. æ„å»ºå¤šç»´åº¦æŸ¥è¯¢åˆ—è¡¨ (æ¢å¤äº†è¢«æ³¨é‡Šçš„å†…å®¹)
        queries = [
            f"{name}",
            f"{translated_name}"
        ]
        
        print(f"ğŸ” æ‰§è¡Œå¤šè·¯æ£€ç´¢, Query æ•°é‡: {len(queries)}")
        
        # 3. æ£€ç´¢ (è¿”å› NodeWithScore åˆ—è¡¨)
        retrieved_nodes = self.retriever.retrieve(query_name=name, original_query=queries)
        
        # 4. æ„å»ºä¸Šä¸‹æ–‡ (ä½¿ç”¨ _build_context_str)
        context_str = self._build_context_str(retrieved_nodes)
        
        print(f"ğŸ“„ æœ€ç»ˆæ„å»ºä¸Šä¸‹æ–‡é•¿åº¦: {len(context_str)} å­—ç¬¦")
        if len(context_str) < 50:
            print("âš ï¸ è­¦å‘Š: æœªæ£€ç´¢åˆ°æœ‰æ•ˆå†…å®¹ï¼")
        
        # 5. æå–ä¿¡æ¯
        profile_data = self.extract_info(name, context_str)
        return profile_data